{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "    \n",
    "1. Import datasets\n",
    "\n",
    "2. Data cleaning + export cleaned datasets to `datasets/cleaned` (Note: index column name should always be \"airport_code\")\n",
    "\n",
    "    2.1 Clean `datasets/original/crash` (unfinished data1)\n",
    "    \n",
    "    2.2 Clean `datasets/original/delay/2009.csv`. Export `unpleasant_2009.csv`.\n",
    "    \n",
    "    2.3 Clean `datasets/original/airport/airport-extended.csv`. Generate `airport_loc_df` with column names city_name, latitude, longitude, altitude_ft.\n",
    "    \n",
    "    2.4 Clean `datasets/original/city/uscities.csv`.\n",
    "    \n",
    "    2.5 Merge `unpleasant_2009` and `airport_loc_df` to get pre-`airport_prop_df`. Drop rows whose city_name isn't included in `us_cities_df`. Scrape pcp data with datasets in `datasets/original/weather`. Then we get `airport_prop_df`. Export `airport_prop_df.csv`.\n",
    "    \n",
    "    2.6 Clean `datasets/original/airports/Bird Strikes.xlsx`. (unfinished)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 2.1 ~ 2.5 are from `A Pleasant Flight.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan as Nan\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from string import digits\n",
    "import wget # you need to \"pip install wget\"  \n",
    "import glob\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go # use conda to install plotly --YD\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "import xml.etree.ElementTree as ET # to read one dataset in XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `A Pleasant Flight.ipynb` (now in the folder `previous_codes`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_df = pd.read_csv(\"datasets/original/crash/Airplane_Crashes_and_Fatalities_Since_1908.csv\") #data1\n",
    "etree = ET.parse(\"datasets/original/crash/AviationData.xml\") #data2\n",
    "delay_2009_df = pd.read_csv(\"datasets/original/delay/2009.csv\") #data3 (done)\n",
    "    #need to download 2009.csv from https://www.kaggle.com/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df = pd.read_csv(\"datasets/original/airport/airports-extended.csv\") #data4\n",
    "#https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals#airports-extended.csv\n",
    "\n",
    "#aiport_code: city_name, latitude, longitude, altitude_ft\n",
    "us_cities_df = pd.read_csv(\"datasets/original/city/uscities.csv\") #data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Clean `datasets/original/crash` (unfinished data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_root = etree.getroot()\n",
    "\n",
    "interest_columns = [\"EventId\",\"EventDate\",\"Location\",\"Country\",\"Latitude\",\"Longitude\",\"AirportCode\",\"InjurySeverity\",\"AircraftDamage\",\n",
    "                    \"AircraftCategory\",\"NumberOfEngines\",\"EngineType\",\"Schedule\",\"TotalUninjured\",\"TotalMinorInjuries\",\n",
    "                    \"TotalSeriousInjuries\",\"TotalFatalInjuries\",\"WeatherCondition\",\"BroadPhaseOfFlight\",\"RegistrationNumber\",\"PurposeOfFlight\"]\n",
    "\n",
    "NTSB_crash_df = pd.DataFrame(columns=interest_columns) # NTSB_crash_df: dataframe which collects airport information. -- YD 02/28/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in xml_root: # This loop will run only once\n",
    "    for row in elem: \n",
    "        if not(row.attrib[\"PurposeOfFlight\"]==\"Business\"):  # Only care about business flights whose engine type is not 'Reciprocating'-- YD 02/28/2020\n",
    "            continue\n",
    "        if row.attrib[\"EngineType\"]==\"Reciprocating\": \n",
    "            continue\n",
    "        information = list()\n",
    "        for interest in interest_columns:\n",
    "            if not (row.attrib[interest]==\"\"):\n",
    "                information.append(row.attrib[interest])\n",
    "            else:\n",
    "                information.append(np.nan)\n",
    "        row_information = pd.Series(information,index=interest_columns)\n",
    "        NTSB_crash_df = NTSB_crash_df.append(row_information,ignore_index=True)\n",
    "            \n",
    "# this may need to run for a while. It takes 18 seconds on my computer\n",
    "#     NTSB_crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTSB_crash_df[(NTSB_crash_df[\"AircraftCategory\"]==\"Airplane\") & (NTSB_crash_df[\"EngineType\"]!=\"Reciprocating\") & (NTSB_crash_df[\"Country\"]==\"United States\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (unfinished Crash Data Cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Clean `datasets/original/delay/2009.csv`. Export `unpleasant_2009.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPARTURE\n",
    "unpleasant_2009_departure = pd.DataFrame()\n",
    "unpleasant_2009_departure['total_departure'] = delay_2009_df.loc[:,[\"ORIGIN\"]].groupby('ORIGIN').size()\n",
    "unpleasant_2009_departure[[\"average_departure_delay\",\"average_departure_taxi\"]] = delay_2009_df.loc[:,[\"ORIGIN\",\"DEP_DELAY\",\"TAXI_OUT\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2009_departure['average_departure_cancelled'] = delay_2009_df.loc[:,[\"ORIGIN\",\"CANCELLED\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2009_departure['averge_departure_distance'] = delay_2009_df.loc[:,[\"ORIGIN\",\"DISTANCE\"]].groupby('ORIGIN').mean()\n",
    "    \n",
    "#ARRIVAL\n",
    "unpleasant_2009_arrival = pd.DataFrame()\n",
    "unpleasant_2009_arrival['total_arrival'] = delay_2009_df.loc[:,[\"DEST\"]].groupby('DEST').size()\n",
    "unpleasant_2009_arrival[[\"average_arrival_delay\",\"average_arrival_taxi\"]] = delay_2009_df.loc[:,[\"DEST\",\"ARR_DELAY\",\"TAXI_IN\"]].groupby('DEST').mean()\n",
    "unpleasant_2009_arrival['average_arrival_diverted'] = delay_2009_df.loc[:,[\"DEST\",\"DIVERTED\"]].groupby('DEST').mean()\n",
    "unpleasant_2009_arrival['averge_arrival_distance'] = delay_2009_df.loc[:,[\"DEST\",\"DISTANCE\"]].groupby('DEST').mean()\n",
    "    \n",
    "unpleasant_2009_departure['total_departure_lg10'] = unpleasant_2009_departure['total_departure'].apply(np.log10)\n",
    "unpleasant_2009_arrival['total_arrival_lg10'] = unpleasant_2009_arrival['total_arrival'].apply(np.log10)\n",
    "\n",
    "unpleasant_2009 = unpleasant_2009_departure.merge(unpleasant_2009_arrival,left_index=True,right_index=True)\n",
    "unpleasant_2009.index.names = [\"airport_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpleasant_2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export `unpleasant_2009.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpleasant_2009.to_csv(\"datasets/cleaned/unpleasant_2009.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Clean `datasets/original/airport/airport-extended.csv`. Generate `airport_loc_df` with column names city_name, latitude, longitude, altitude_ft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean city_name\n",
    "def clean_city_name(input_city):\n",
    "    original = input_city\n",
    "    input_city = str(input_city)\n",
    "    input_city = input_city.strip()\n",
    "    input_city = input_city.lower()\n",
    "    \n",
    "    input_city = input_city.replace(\".\",\"\")\n",
    "    input_city = input_city.replace(\"\\\\\\\\\",\"\")\n",
    "    input_city = input_city.replace(\"-\",\" \")\n",
    "    input_city = input_city.replace(\" - \",\" \")\n",
    "    input_city = input_city.replace(\"saint \",\"st\")\n",
    "    input_city = input_city.replace(\"east \",\"\")\n",
    "    input_city = input_city.replace(\"west \",\"\")\n",
    "    \n",
    "    input_city = input_city.translate({ord(k): None for k in digits})\n",
    "    \n",
    "    if ('/' in input_city):\n",
    "        input_city = input_city[:input_city.find('/')]\n",
    "    if ('(' in input_city):\n",
    "        input_city = input_city[:input_city.find('(')]\n",
    "    if (',' in input_city):\n",
    "        input_city = input_city[:input_city.find(',')]\n",
    "    input_city = input_city.strip()   \n",
    "    if (' ' in input_city):\n",
    "        temp=input_city.find(' ')\n",
    "        if (temp > 2):\n",
    "            input_city = input_city[:input_city.find(' ')]\n",
    "        else:\n",
    "            if (input_city.find(' ',temp+1) != -1):\n",
    "                input_city = input_city[temp+1:input_city.find(' ',temp+1)]\n",
    "            else:\n",
    "                input_city = input_city[temp+1:]\n",
    "    input_city = input_city.strip()\n",
    "    try:\n",
    "        assert len(input_city) > 2\n",
    "        assert input_city.replace(\" \",\"\").replace(\"'\",\"\").isalpha()\n",
    "    except:\n",
    "        print(\"This city name is prehaps incorrect: \",original,input_city,len(original))\n",
    "    return input_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df.columns = [\"ID\",\"name\",\"city_name\",\"country\",\"airport_code\",\"code4\",\"latitude\",\"longitude\",\"altitude_ft\",\"UTC_offset\",\"DST\",\"timezone\",\"type\",\"information_source\"]\n",
    "airport_loc_df = airport_loc_df.loc[:,[\"city_name\",\"country\",\"airport_code\",\"latitude\",\"longitude\",\"altitude_ft\"]]\n",
    "airport_loc_df = airport_loc_df[airport_loc_df[\"country\"]==\"United States\"]\n",
    "airport_loc_df = airport_loc_df.loc[:,[\"city_name\",\"airport_code\",\"latitude\",\"longitude\",\"altitude_ft\"]]\n",
    "airport_loc_df = airport_loc_df[airport_loc_df[\"airport_code\"]!=\"\\\\N\"] # remove NAN in index\n",
    "airport_loc_df = airport_loc_df.set_index(\"airport_code\")\n",
    "\n",
    "airport_loc_df[\"city_name\"] = airport_loc_df[\"city_name\"].apply(clean_city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airport_loc_df.to_csv(\"datasets/cleaned/airport_loc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Clean `datasets/original/city/uscities.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df = us_cities_df[[\"city\",\"state_id\",\"county_fips\",\"county_name\",\"population\",\"density\",\"lat\",\"lng\"]]\n",
    "us_cities_df = us_cities_df.rename(columns = {\"city\":\"city_name\"})\n",
    "us_cities_df[\"fips\"] = us_cities_df[\"county_fips\"]\n",
    "\n",
    "def get_county_code(input_county):\n",
    "    return int(input_county) % 1000\n",
    "\n",
    "us_cities_df[\"county_fips\"] = us_cities_df[\"county_fips\"].apply(get_county_code)\n",
    "\n",
    "us_cities_df[\"city_name\"] = us_cities_df[\"city_name\"].apply(clean_city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Merge `unpleasant_2009` and `airport_loc_df` to get pre-`airport_prop_df`. Drop rows whose city_name isn't included in `us_cities_df`. Scrape pcp data with datasets in `datasets/original/weather`. Then we get `airport_prop_df`. Export airport_prop_df.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df = unpleasant_2009.merge(airport_loc_df,how='inner',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airport_code_df = airport_prop_df[[]]\n",
    "#airport_code_df\n",
    "#airport_code_df.to_csv(\"datasets/cleaned/airtport_code.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df = airport_prop_df[[\"city_name\", \"latitude\", \"longitude\", \"altitude_ft\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_climate_df = pd.DataFrame(columns=[\"airport_code\",\"population\",\"density\",\"avg_temp_sp\",\"avg_temp_su\",\"avg_temp_fa\",\"avg_temp_wi\",\"avg_precipitation_sp\",\"avg_precipitation_su\",\"avg_precipitation_fa\",\"avg_precipitation_wi\"])\n",
    "city_search_df = pd.DataFrame(columns=[\"airport_code\",\"state_id\",\"county_id\",\"city_id\",\"fips\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,row in airport_prop_df.iterrows():\n",
    "    city = row[\"city_name\"]\n",
    "    target_lat = row[\"latitude\"]\n",
    "    target_lng = row[\"longitude\"]\n",
    "    try:      \n",
    "        target_cities = us_cities_df[us_cities_df[\"city_name\"]==city]\n",
    "        \n",
    "        if not (target_cities.shape[0] == 1):\n",
    "            def calc_dis(input_):\n",
    "                err = abs(target_lat - input_[\"lat\"]) + abs(target_lng - input_[\"lng\"])\n",
    "                return err\n",
    "            target_cities.loc[:,\"error\"] = (target_cities.apply(calc_dis,axis=1))\n",
    "            target_city = target_cities.sort_values(by=\"error\").iloc[0]\n",
    "            \n",
    "            assert target_city[\"error\"] < 1.5\n",
    "            \n",
    "            target_city = target_city.drop([\"error\"])\n",
    "        elif (target_cities.shape[0] >= 1):\n",
    "            target_city = target_cities.iloc[0]\n",
    "        \n",
    "        county = str(target_city[\"county_fips\"])\n",
    "        if (len(county)==1):\n",
    "            county = \"00\" + county\n",
    "        elif (len(county)==2):\n",
    "            county = \"0\" + county\n",
    "\n",
    "        city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":target_city[\"state_id\"],\"county_id\":county,\"city_id\":target_city.name,\"fips\":target_city[\"fips\"]},ignore_index=True)        \n",
    "    except:\n",
    "        try:\n",
    "            def calc_dis(input_):\n",
    "                err = abs(target_lat - input_[\"lat\"]) + abs(target_lng - input_[\"lng\"])\n",
    "                return err\n",
    "            us_cities_df_copy = us_cities_df\n",
    "            us_cities_df_copy.loc[:,\"error\"] = (us_cities_df.apply(calc_dis,axis=1))\n",
    "            target_city = us_cities_df_copy.sort_values(by=\"error\").iloc[0]\n",
    "            assert target_city[\"error\"] < 1.5\n",
    "            county = str(target_city[\"county_fips\"])\n",
    "            if (len(county)==1):\n",
    "                county = \"00\" + county\n",
    "            elif (len(county)==2):\n",
    "                county = \"0\" + county\n",
    "            city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":target_city[\"state_id\"],\"county_id\":county,\"city_id\":target_city.name,\"fips\":target_city[\"fips\"]},ignore_index=True)\n",
    "        except:\n",
    "            print(\"No data for \",city)\n",
    "            city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":np.nan,\"county_id\":np.nan,\"city_id\":np.nan,\"fips\":np.nan},ignore_index=True)\n",
    "\n",
    "#special case for DC\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    if (row[\"airport_code\"]==\"DCA\"):\n",
    "        city_search_df.iloc[ind][\"state_id\"]=\"MD\"\n",
    "        city_search_df.iloc[ind][\"county_id\"]=\"511\"\n",
    "\n",
    "        \n",
    "        \n",
    "city_search_df = city_search_df[(city_search_df[\"state_id\"]!=\"HI\") & (city_search_df[\"state_id\"]!=\"AK\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_search_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape cities in `city_search_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_climate_data(state,county,year):\n",
    "    save_path = \"datasets/original/weather/\"\n",
    "    fname = state + county + \"_\" + str(year) + \".csv\"\n",
    "    if (len(glob.glob(save_path + fname))==0):\n",
    "        URL = \"https://www.ncdc.noaa.gov/cag/county/time-series/{}-{}-{}-all-1-2000-2020.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state,county,\"tavg\")\n",
    "        r = requests.get(URL)\n",
    "        file = wget.download(URL,out=save_path + \"tavg/tavg_\" + fname)\n",
    "        URL = \"https://www.ncdc.noaa.gov/cag/county/time-series/{}-{}-{}-all-1-2000-2020.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state,county,\"pcp\")\n",
    "        r = requests.get(URL)\n",
    "        file = wget.download(URL,out=save_path + \"pcp/pcp_\" + fname)\n",
    "\n",
    "        tavg_df = pd.read_csv(save_path + \"tavg/tavg_\" + fname).iloc[4:]\n",
    "        tavg_df.columns=[\"date\",\"tavg\",\"comp\"]\n",
    "        tavg = tavg_df.set_index(\"date\")[\"tavg\"]\n",
    "\n",
    "        pcp_df = pd.read_csv(save_path + \"pcp/pcp_\" + fname).iloc[4:]\n",
    "        pcp_df.columns=[\"date\",\"pcp\",\"comp\"]\n",
    "        pcp = pcp_df.set_index(\"date\")[\"pcp\"]\n",
    "\n",
    "        pd.concat([tavg, pcp], axis=1).to_csv(save_path + fname)\n",
    "        time.sleep(1) # not requesting too frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    try:\n",
    "        download_climate_data(row[\"state_id\"],row[\"county_id\"],2018)\n",
    "        download_climate_data(row[\"state_id\"],row[\"county_id\"],2019)\n",
    "    except:\n",
    "        print(row)\n",
    "    counter+=1\n",
    "    print(\"progress: {:.2f}%   Just done: {}\".format(100 * counter / city_search_df.shape[0],row[\"airport_code\"]),end=\"\\r\")\n",
    "\n",
    "\n",
    "temp_pcp_df = pd.DataFrame(columns=[\"airport_code\",\"temp_avg\",\"pcp_avg\"])\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    state = row[\"state_id\"]\n",
    "    county = row[\"county_id\"]\n",
    "    save_path = \"datasets/original/weather/\"\n",
    "    years = [2018,2019]\n",
    "    \n",
    "    try:\n",
    "        tavg = 0\n",
    "        pcp = 0\n",
    "        for year in years:\n",
    "            fname = state + county + \"_\" + str(year) + \".csv\"\n",
    "            temp_pcp = pd.read_csv(save_path + fname)\n",
    "            tavg += temp_pcp.mean()[\"tavg\"]\n",
    "            pcp += temp_pcp.mean()[\"pcp\"]\n",
    "        tavg /= len(years)\n",
    "        pcp /= len(years)\n",
    "        \n",
    "        temp_pcp_df = temp_pcp_df.append({\"airport_code\":row[\"airport_code\"],\"temp_avg\":tavg,\"pcp_avg\":pcp},ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        temp_pcp_df = temp_pcp_df.append({\"airport_code\":row[\"airport_code\"],\"temp_avg\":np.nan,\"pcp_avg\":np.nan},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df = airport_prop_df.merge(city_search_df.set_index(\"airport_code\").loc[:,[\"city_id\",\"fips\"]],left_index=True,right_index=True)\n",
    "airport_prop_df = airport_prop_df.merge(temp_pcp_df.set_index(\"airport_code\"),left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df.to_csv(\"datasets/cleaned/airport_prop.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Clean `datasets/original/airports/Bird Strikes.xlsx`. (unfinished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

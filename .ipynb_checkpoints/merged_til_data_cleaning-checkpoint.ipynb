{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "    \n",
    "1. Import datasets\n",
    "\n",
    "2. Data cleaning + export cleaned datasets to `datasets/cleaned` (Note: index column name should always be \"airport_code\")\n",
    "\n",
    "    2.1 Clean `datasets/original/crash` (unfinished data1)\n",
    "    \n",
    "    2.2 Clean `datasets/original/delay`. Generate `unpleasant_2009.csv`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan as Nan\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from string import digits\n",
    "import wget # you need to \"pip install wget\"  \n",
    "import glob\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go # use conda to install plotly --YD\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "import xml.etree.ElementTree as ET # to read one dataset in XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pleasent Flight\n",
    "crash_df = pd.read_csv(\"datasets/original/crash/Airplane_Crashes_and_Fatalities_Since_1908.csv\") #data1\n",
    "etree = ET.parse(\"datasets/original/crash/AviationData.xml\") #data2\n",
    "delay_2009_df = pd.read_csv(\"datasets/original/delay/2009.csv\") #data3 (done)\n",
    "    #need to download 2009.csv from https://www.kaggle.com/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Clean `datasets/original/crash`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_root = etree.getroot()\n",
    "\n",
    "interest_columns = [\"EventId\",\"EventDate\",\"Location\",\"Country\",\"Latitude\",\"Longitude\",\"AirportCode\",\"InjurySeverity\",\"AircraftDamage\",\n",
    "                    \"AircraftCategory\",\"NumberOfEngines\",\"EngineType\",\"Schedule\",\"TotalUninjured\",\"TotalMinorInjuries\",\n",
    "                    \"TotalSeriousInjuries\",\"TotalFatalInjuries\",\"WeatherCondition\",\"BroadPhaseOfFlight\",\"RegistrationNumber\",\"PurposeOfFlight\"]\n",
    "\n",
    "NTSB_crash_df = pd.DataFrame(columns=interest_columns) # NTSB_crash_df: dataframe which collects airport information. -- YD 02/28/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in xml_root: # This loop will run only once\n",
    "    for row in elem: \n",
    "        if not(row.attrib[\"PurposeOfFlight\"]==\"Business\"):  # Only care about business flights whose engine type is not 'Reciprocating'-- YD 02/28/2020\n",
    "            continue\n",
    "        if row.attrib[\"EngineType\"]==\"Reciprocating\": \n",
    "            continue\n",
    "        information = list()\n",
    "        for interest in interest_columns:\n",
    "            if not (row.attrib[interest]==\"\"):\n",
    "                information.append(row.attrib[interest])\n",
    "            else:\n",
    "                information.append(np.nan)\n",
    "        row_information = pd.Series(information,index=interest_columns)\n",
    "        NTSB_crash_df = NTSB_crash_df.append(row_information,ignore_index=True)\n",
    "            \n",
    "# this may need to run for a while. It takes 18 seconds on my computer\n",
    "#     NTSB_crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTSB_crash_df[(NTSB_crash_df[\"AircraftCategory\"]==\"Airplane\") & (NTSB_crash_df[\"EngineType\"]!=\"Reciprocating\") & (NTSB_crash_df[\"Country\"]==\"United States\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (unfinished Crash Data Cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Clean `datasets/original/delay`. Generate `unpleasant_2009.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPARTURE\n",
    "unpleasant_2009_departure = pd.DataFrame()\n",
    "unpleasant_2009_departure['total_departure'] = delay_2009_df.loc[:,[\"ORIGIN\"]].groupby('ORIGIN').size()\n",
    "unpleasant_2009_departure[[\"average_departure_delay\",\"average_departure_taxi\"]] = delay_2009_df.loc[:,[\"ORIGIN\",\"DEP_DELAY\",\"TAXI_OUT\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2009_departure['average_departure_cancelled'] = delay_2009_df.loc[:,[\"ORIGIN\",\"CANCELLED\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2009_departure['averge_departure_distance'] = delay_2009_df.loc[:,[\"ORIGIN\",\"DISTANCE\"]].groupby('ORIGIN').mean()\n",
    "    \n",
    "#ARRIVAL\n",
    "unpleasant_2009_arrival = pd.DataFrame()\n",
    "unpleasant_2009_arrival['total_arrival'] = delay_2009_df.loc[:,[\"DEST\"]].groupby('DEST').size()\n",
    "unpleasant_2009_arrival[[\"average_arrival_delay\",\"average_arrival_taxi\"]] = delay_2009_df.loc[:,[\"DEST\",\"ARR_DELAY\",\"TAXI_IN\"]].groupby('DEST').mean()\n",
    "unpleasant_2009_arrival['average_arrival_diverted'] = delay_2009_df.loc[:,[\"DEST\",\"DIVERTED\"]].groupby('DEST').mean()\n",
    "unpleasant_2009_arrival['averge_arrival_distance'] = delay_2009_df.loc[:,[\"DEST\",\"DISTANCE\"]].groupby('DEST').mean()\n",
    "    \n",
    "unpleasant_2009_departure['total_departure_lg10'] = unpleasant_2009_departure['total_departure'].apply(np.log10)\n",
    "unpleasant_2009_arrival['total_arrival_lg10'] = unpleasant_2009_arrival['total_arrival'].apply(np.log10)\n",
    "\n",
    "unpleasant_2009 = unpleasant_2009_departure.merge(unpleasant_2009_arrival,left_index=True,right_index=True)\n",
    "unpleasant_2009.index.names = [\"airport_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export `unpleasant_2009.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpleasant_2009.to_csv(\"datasets/cleaned/unpleasant_2009.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

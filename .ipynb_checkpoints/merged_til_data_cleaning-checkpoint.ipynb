{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "    \n",
    "1. Import datasets\n",
    "\n",
    "2. Data cleaning + export cleaned datasets to `datasets/cleaned` (Note: index column name should always be \"airport_code\")\n",
    "\n",
    "    2.1 Clean `datasets/original/crash` (unfinished data1)\n",
    "    \n",
    "    2.2 Clean `datasets/original/delay/2009.csv`. Generate `unpleasant_2009.csv`\n",
    "    \n",
    "    2.3 Clean `datasets/original/airport/airport-extended.csv`. Generate `airport_loc.csv`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan as Nan\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from string import digits\n",
    "import wget # you need to \"pip install wget\"  \n",
    "import glob\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go # use conda to install plotly --YD\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "import xml.etree.ElementTree as ET # to read one dataset in XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `A Pleasant Flight.ipynb` (now in the folder `previous_codes`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_df = pd.read_csv(\"datasets/original/crash/Airplane_Crashes_and_Fatalities_Since_1908.csv\") #data1\n",
    "etree = ET.parse(\"datasets/original/crash/AviationData.xml\") #data2\n",
    "delay_2009_df = pd.read_csv(\"datasets/original/delay/2009.csv\") #data3 (done)\n",
    "    #need to download 2009.csv from https://www.kaggle.com/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df = pd.read_csv(\"datasets/original/airport/airports-extended.csv\") #data4\n",
    "#https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals#airports-extended.csv\n",
    "\n",
    "#aiport_code: city_name, latitude, longitude, altitude_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Clean `datasets/original/crash`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_root = etree.getroot()\n",
    "\n",
    "interest_columns = [\"EventId\",\"EventDate\",\"Location\",\"Country\",\"Latitude\",\"Longitude\",\"AirportCode\",\"InjurySeverity\",\"AircraftDamage\",\n",
    "                    \"AircraftCategory\",\"NumberOfEngines\",\"EngineType\",\"Schedule\",\"TotalUninjured\",\"TotalMinorInjuries\",\n",
    "                    \"TotalSeriousInjuries\",\"TotalFatalInjuries\",\"WeatherCondition\",\"BroadPhaseOfFlight\",\"RegistrationNumber\",\"PurposeOfFlight\"]\n",
    "\n",
    "NTSB_crash_df = pd.DataFrame(columns=interest_columns) # NTSB_crash_df: dataframe which collects airport information. -- YD 02/28/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in xml_root: # This loop will run only once\n",
    "    for row in elem: \n",
    "        if not(row.attrib[\"PurposeOfFlight\"]==\"Business\"):  # Only care about business flights whose engine type is not 'Reciprocating'-- YD 02/28/2020\n",
    "            continue\n",
    "        if row.attrib[\"EngineType\"]==\"Reciprocating\": \n",
    "            continue\n",
    "        information = list()\n",
    "        for interest in interest_columns:\n",
    "            if not (row.attrib[interest]==\"\"):\n",
    "                information.append(row.attrib[interest])\n",
    "            else:\n",
    "                information.append(np.nan)\n",
    "        row_information = pd.Series(information,index=interest_columns)\n",
    "        NTSB_crash_df = NTSB_crash_df.append(row_information,ignore_index=True)\n",
    "            \n",
    "# this may need to run for a while. It takes 18 seconds on my computer\n",
    "#     NTSB_crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTSB_crash_df[(NTSB_crash_df[\"AircraftCategory\"]==\"Airplane\") & (NTSB_crash_df[\"EngineType\"]!=\"Reciprocating\") & (NTSB_crash_df[\"Country\"]==\"United States\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (unfinished Crash Data Cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Clean `datasets/original/delay`. Generate `unpleasant_2009.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPARTURE\n",
    "unpleasant_2009_departure = pd.DataFrame()\n",
    "unpleasant_2009_departure['total_departure'] = delay_2009_df.loc[:,[\"ORIGIN\"]].groupby('ORIGIN').size()\n",
    "unpleasant_2009_departure[[\"average_departure_delay\",\"average_departure_taxi\"]] = delay_2009_df.loc[:,[\"ORIGIN\",\"DEP_DELAY\",\"TAXI_OUT\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2009_departure['average_departure_cancelled'] = delay_2009_df.loc[:,[\"ORIGIN\",\"CANCELLED\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2009_departure['averge_departure_distance'] = delay_2009_df.loc[:,[\"ORIGIN\",\"DISTANCE\"]].groupby('ORIGIN').mean()\n",
    "    \n",
    "#ARRIVAL\n",
    "unpleasant_2009_arrival = pd.DataFrame()\n",
    "unpleasant_2009_arrival['total_arrival'] = delay_2009_df.loc[:,[\"DEST\"]].groupby('DEST').size()\n",
    "unpleasant_2009_arrival[[\"average_arrival_delay\",\"average_arrival_taxi\"]] = delay_2009_df.loc[:,[\"DEST\",\"ARR_DELAY\",\"TAXI_IN\"]].groupby('DEST').mean()\n",
    "unpleasant_2009_arrival['average_arrival_diverted'] = delay_2009_df.loc[:,[\"DEST\",\"DIVERTED\"]].groupby('DEST').mean()\n",
    "unpleasant_2009_arrival['averge_arrival_distance'] = delay_2009_df.loc[:,[\"DEST\",\"DISTANCE\"]].groupby('DEST').mean()\n",
    "    \n",
    "unpleasant_2009_departure['total_departure_lg10'] = unpleasant_2009_departure['total_departure'].apply(np.log10)\n",
    "unpleasant_2009_arrival['total_arrival_lg10'] = unpleasant_2009_arrival['total_arrival'].apply(np.log10)\n",
    "\n",
    "unpleasant_2009 = unpleasant_2009_departure.merge(unpleasant_2009_arrival,left_index=True,right_index=True)\n",
    "unpleasant_2009.index.names = [\"airport_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpleasant_2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export `unpleasant_2009.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unpleasant_2009' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a93c1b43c19c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munpleasant_2009\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datasets/cleaned/unpleasant_2009.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'unpleasant_2009' is not defined"
     ]
    }
   ],
   "source": [
    "unpleasant_2009.to_csv(\"datasets/cleaned/unpleasant_2009.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean city_name\n",
    "def clean_city_name(input_city):\n",
    "    original = input_city\n",
    "    input_city = str(input_city)\n",
    "    input_city = input_city.strip()\n",
    "    input_city = input_city.lower()\n",
    "    \n",
    "    input_city = input_city.replace(\".\",\"\")\n",
    "    input_city = input_city.replace(\"\\\\\\\\\",\"\")\n",
    "    input_city = input_city.replace(\"-\",\" \")\n",
    "    input_city = input_city.replace(\" - \",\" \")\n",
    "    input_city = input_city.replace(\"saint \",\"st\")\n",
    "    input_city = input_city.replace(\"east \",\"\")\n",
    "    input_city = input_city.replace(\"west \",\"\")\n",
    "    \n",
    "    input_city = input_city.translate({ord(k): None for k in digits})\n",
    "    \n",
    "    if ('/' in input_city):\n",
    "        input_city = input_city[:input_city.find('/')]\n",
    "    if ('(' in input_city):\n",
    "        input_city = input_city[:input_city.find('(')]\n",
    "    if (',' in input_city):\n",
    "        input_city = input_city[:input_city.find(',')]\n",
    "    input_city = input_city.strip()   \n",
    "    if (' ' in input_city):\n",
    "        temp=input_city.find(' ')\n",
    "        if (temp > 2):\n",
    "            input_city = input_city[:input_city.find(' ')]\n",
    "        else:\n",
    "            if (input_city.find(' ',temp+1) != -1):\n",
    "                input_city = input_city[temp+1:input_city.find(' ',temp+1)]\n",
    "            else:\n",
    "                input_city = input_city[temp+1:]\n",
    "    input_city = input_city.strip()\n",
    "    try:\n",
    "        assert len(input_city) > 2\n",
    "        assert input_city.replace(\" \",\"\").replace(\"'\",\"\").isalpha()\n",
    "    except:\n",
    "        print(\"This city name is prehaps incorrect: \",original,input_city,len(original))\n",
    "    return input_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df.columns = [\"ID\",\"name\",\"city_name\",\"country\",\"airport_code\",\"code4\",\"latitude\",\"longitude\",\"altitude_ft\",\"UTC_offset\",\"DST\",\"timezone\",\"type\",\"information_source\"]\n",
    "airport_loc_df = airport_loc_df.loc[:,[\"city_name\",\"country\",\"airport_code\",\"latitude\",\"longitude\",\"altitude_ft\"]]\n",
    "airport_loc_df = airport_loc_df[airport_loc_df[\"country\"]==\"United States\"]\n",
    "airport_loc_df = airport_loc_df.loc[:,[\"city_name\",\"airport_code\",\"latitude\",\"longitude\",\"altitude_ft\"]]\n",
    "airport_loc_df = airport_loc_df[airport_loc_df[\"airport_code\"]!=\"\\\\N\"] # remove NAN in index\n",
    "airport_loc_df = airport_loc_df.set_index(\"airport_code\")\n",
    "\n",
    "airport_loc_df[\"city_name\"] = airport_loc_df[\"city_name\"].apply(clean_city_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export `airport_loc.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df.to_csv(\"datasets/cleaned/airport_loc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

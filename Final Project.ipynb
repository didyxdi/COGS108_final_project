{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Ivan Jin (A14880280)\n",
    " - Hongsheng Xie (A14794772)\n",
    " - Tong Wang (A13713688)\n",
    " - Yinxuan Du (A15873678)\n",
    " - Yuchen Zhang (A16151373)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Question Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background & Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BG + Prior work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Airline Delay and Cancellation Data, 2009 - 2018\n",
    "- Content: US domestic flight delay data\n",
    "- Columns of interest: \n",
    "- \\# of observations: 6429338 rows for year 2009\n",
    "- Source: https://www.kaggle.com/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018\n",
    "- Function: main outcome variable\n",
    "\n",
    "### 2. FILL ME ACCORDING TO OUR DATASET!\n",
    "- Content:\n",
    "- Columns of interest: \n",
    "- \\# of observations: \n",
    "- Source: \n",
    "- Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan as Nan\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from string import digits\n",
    "import wget \n",
    "import glob\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "import xml.etree.ElementTree as ET # to read one dataset in XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer our question, we need to clean and aggregate our main outcome variable (delay, crash) and our predictors by airports. The goal of data cleaning is to get two dataframes that have airport code as index:\n",
    " - 1.Y \n",
    "     - columns\n",
    "         - average_departure_delay - average delay of all flights originated from this airport \n",
    "         - average_arrival_delay - average delay of all flights arriving at this airport\n",
    "         - average_cancelled - average cancel rate of all flights originated from this airport\n",
    "         - average_diverted - average diverted rate of all flights arriving at this airport\n",
    " - 2.X \n",
    "     - columns\n",
    "         - code4 - 4 digit airport code\n",
    "         - city_name - name of the city that located near the airport\n",
    "         - city_id - index of this city in uscities_df\n",
    "         - latitude - latitude of the airport\n",
    "         - longitude - longitude of the airport\n",
    "         - altitude_ft - altitude of the airport\n",
    "         - temp_avg - 2018-2019 average tempreature of the county contining the airport\n",
    "         - pcp_avg - 2018-2019 average precipitation of the county contining the airport\n",
    "         - strike_avg - year-average number bird strike (may not caused damage)\n",
    "         - damage_avg - year-average number of bird strike that caused damage\n",
    "         - city_population - population the city that located near the airport\n",
    "         - enplanements - enplanements of the city that located near the airport\n",
    "         - length_ft_sum - sum of length of all runways\n",
    "         - width_ft_avg - average width of all runways\n",
    "         - num_runways - number of runways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning - Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Delay dataset - departure/ arrival delay + cancel / divert rate \n",
    "From `A Pleasant Flight.ipynb` (now in the folder `previous_codes`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_2018_df = pd.read_csv(\"datasets/original/delay/2018.csv\")\n",
    "delay_2018_df = delay_2018_df[[\"ORIGIN\",\"DEST\",\"DEP_DELAY\",\"TAXI_OUT\",\"CANCELLED\",\"DISTANCE\",\"ARR_DELAY\",\"TAXI_IN\",\"DIVERTED\"]] # columns of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,we check the distribution of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ORIGIN            0\n",
       "DEST              0\n",
       "DEP_DELAY    117234\n",
       "TAXI_OUT     115830\n",
       "CANCELLED         0\n",
       "DISTANCE          0\n",
       "ARR_DELAY    137040\n",
       "TAXI_IN      119246\n",
       "DIVERTED          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_2018_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values for delay columns. However, we discovered that 95% of the missing value have \"CANCALLED\" == 1. It is logical to have missing delay value when the flight is never done. For these flight have already contributed to the cancel rate feature, thus should not have a effect on the delay. Since we are using the .mean() function, which will ignore all nan values, we left these rows unchanged. Same for diverted, when there shouldn't be a arrival delay because the flight is not arriving at the scheduled airport. We kept them because we can have a more accurate total departure/arrival number. In this way we have only 4000 rows that we can't explain the reason behind missing value. Since we have 7213446 rows in total, 4000 is a acceptable amount of missing. We also kept it for accurate total departure/arrival number.\n",
    "We then calculated departure_delay, arrival_delay and other columns of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPARTURE\n",
    "unpleasant_2018_departure = pd.DataFrame()\n",
    "unpleasant_2018_departure['total_departure'] = delay_2018_df.loc[:,[\"ORIGIN\"]].groupby('ORIGIN').size()\n",
    "unpleasant_2018_departure[[\"average_departure_delay\",\"average_departure_taxi\"]] = delay_2018_df.loc[:,[\"ORIGIN\",\"DEP_DELAY\",\"TAXI_OUT\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2018_departure['average_departure_cancelled'] = delay_2018_df.loc[:,[\"ORIGIN\",\"CANCELLED\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2018_departure['averge_departure_distance'] = delay_2018_df.loc[:,[\"ORIGIN\",\"DISTANCE\"]].groupby('ORIGIN').mean()\n",
    "    \n",
    "#ARRIVAL\n",
    "unpleasant_2018_arrival = pd.DataFrame()\n",
    "unpleasant_2018_arrival['total_arrival'] = delay_2018_df.loc[:,[\"DEST\"]].groupby('DEST').size()\n",
    "unpleasant_2018_arrival[[\"average_arrival_delay\",\"average_arrival_taxi\"]] = delay_2018_df.loc[:,[\"DEST\",\"ARR_DELAY\",\"TAXI_IN\"]].groupby('DEST').mean()\n",
    "unpleasant_2018_arrival['average_arrival_diverted'] = delay_2018_df.loc[:,[\"DEST\",\"DIVERTED\"]].groupby('DEST').mean()\n",
    "unpleasant_2018_arrival['averge_arrival_distance'] = delay_2018_df.loc[:,[\"DEST\",\"DISTANCE\"]].groupby('DEST').mean()\n",
    "\n",
    "#Add log to total_departure and total_arrival \n",
    "unpleasant_2018_departure['total_departure_lg10'] = unpleasant_2018_departure['total_departure'].apply(np.log10)\n",
    "unpleasant_2018_arrival['total_arrival_lg10'] = unpleasant_2018_arrival['total_arrival'].apply(np.log10)\n",
    "\n",
    "#Merge departure and arrival\n",
    "delay_2018_df = unpleasant_2018_departure.merge(unpleasant_2018_arrival,left_index=True,right_index=True)\n",
    "delay_2018_df.index.names = [\"airport_code\"]\n",
    "\n",
    "# Main outcome variable\n",
    "Y = delay_2018_df.loc[:,[\"average_departure_delay\",\"average_arrival_delay\",\"average_departure_cancelled\",\"average_arrival_diverted\"]]\n",
    "\n",
    "#\n",
    "unpleasant_airport_code_df = delay_2018_df[[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning - X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Delay dataset - total arrival / departure + average departure / arrival distance + departure / arrival taxi time\n",
    "From `A Pleasant Flight.ipynb` (now in the folder `previous_codes`), Cleaned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract X features\n",
    "X = pd.DataFrame()\n",
    "X[[\"total_departure\",\"total_arrival\",\"averge_departure_distance\",\"averge_arrival_distance\",\"average_departure_taxi\",\"average_arrival_taxi\"]] \\\n",
    "= delay_2018_df.loc[:,[\"total_departure\",\"total_arrival\",\"averge_departure_distance\",\"averge_arrival_distance\",\"average_departure_taxi\",\"average_arrival_taxi\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 U.S. Airport dataset - city name, latitude, longitude, altitude\n",
    "From `A Pleasant Flight.ipynb` (now in the folder `previous_codes`).\n",
    "\n",
    "Clean `datasets/original/airport/airport-extended.csv`. Generate `airport_loc_df` with column names city_name, latitude, longitude, altitude_ft. Export `airport_loc.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df = pd.read_csv(\"datasets/original/airport/airports-extended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_city_name(input_city):\n",
    "    original = input_city\n",
    "    input_city = str(input_city)\n",
    "    input_city = input_city.strip()\n",
    "    input_city = input_city.lower()\n",
    "    \n",
    "    input_city = input_city.replace(\".\",\"\")\n",
    "    input_city = input_city.replace(\"\\\\\\\\\",\"\")\n",
    "    input_city = input_city.replace(\"-\",\" \")\n",
    "    input_city = input_city.replace(\" - \",\" \")\n",
    "    input_city = input_city.replace(\"saint \",\"st\")\n",
    "    input_city = input_city.replace(\"east \",\"\")\n",
    "    input_city = input_city.replace(\"west \",\"\")\n",
    "    \n",
    "    input_city = input_city.translate({ord(k): None for k in digits})\n",
    "    \n",
    "    if ('/' in input_city):\n",
    "        input_city = input_city[:input_city.find('/')] # in case city have muitiple names like \"cityname1/cityname2\"\n",
    "    if ('(' in input_city):\n",
    "        input_city = input_city[:input_city.find('(')] # Same as above\n",
    "    if (',' in input_city):\n",
    "        input_city = input_city[:input_city.find(',')] \n",
    "    input_city = input_city.strip()   \n",
    "    if (' ' in input_city):\n",
    "        temp=input_city.find(' ')\n",
    "        if (temp > 2):\n",
    "            input_city = input_city[:input_city.find(' ')]\n",
    "        else:\n",
    "            if (input_city.find(' ',temp+1) != -1):\n",
    "                input_city = input_city[temp+1:input_city.find(' ',temp+1)]\n",
    "            else:\n",
    "                input_city = input_city[temp+1:]\n",
    "    input_city = input_city.strip()\n",
    "    try:\n",
    "        assert len(input_city) > 2\n",
    "        assert input_city.replace(\" \",\"\").replace(\"'\",\"\").isalpha()\n",
    "    except:\n",
    "        #print(\"This city name is prehaps incorrect: \",original,input_city,len(original))\n",
    "        1+1\n",
    "    return input_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df.columns = [\"ID\",\"name\",\"city_name\",\"country\",\"airport_code\",\"code4\",\"latitude\",\"longitude\",\"altitude_ft\",\"UTC_offset\",\"DST\",\"timezone\",\"type\",\"information_source\"]\n",
    "airport_loc_df = airport_loc_df.loc[:,[\"city_name\",\"country\",\"airport_code\",\"code4\", \"latitude\",\"longitude\",\"altitude_ft\"]] # add code4 since we need to use it to get runways data --YD 03/05\n",
    "airport_loc_df = airport_loc_df[airport_loc_df[\"country\"]==\"United States\"] # reduce dataframe size\n",
    "airport_loc_df = airport_loc_df.loc[:,[\"city_name\",\"airport_code\",\"code4\", \"latitude\",\"longitude\",\"altitude_ft\"]]# We don't need the country column again\n",
    "airport_loc_df = airport_loc_df[airport_loc_df[\"airport_code\"]!=\"\\\\N\"] # remove NAN in index\n",
    "airport_loc_df = airport_loc_df.set_index(\"airport_code\")\n",
    "airport_loc_df[\"city_name\"] = airport_loc_df[\"city_name\"].apply(clean_city_name)# clean city name\n",
    "\n",
    "# store cleaned data\n",
    "airport_loc_df.to_csv(\"datasets/cleaned/airport_loc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 U.S. Cities dataset - city name, FIPS, population\n",
    "Clean `datasets/original/city/uscities.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df = pd.read_csv(\"datasets/original/city/uscities.csv\")\n",
    "\n",
    "# Kept latitude and longitude to differentiate cities with same name\n",
    "us_cities_df = us_cities_df[[\"city\",\"state_id\",\"county_fips\",\"county_name\",\"population\",\"lat\",\"lng\"]] \n",
    "us_cities_df = us_cities_df.rename(columns = {\"city\":\"city_name\"})                                              \n",
    "us_cities_df[\"fips\"] = us_cities_df[\"county_fips\"]\n",
    "\n",
    "def get_county_code(input_county):\n",
    "    return int(input_county) % 1000\n",
    "\n",
    "us_cities_df[\"county_fips\"] = us_cities_df[\"county_fips\"].apply(get_county_code)\n",
    "us_cities_df[\"city_name\"] = us_cities_df[\"city_name\"].apply(clean_city_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Merge step 2.2 - 2.4\n",
    "Merge `unpleasant_airport_code_df`(empty dataframe with only `airport_code` as index) and `airport_loc_df` by `airport_code` to select the airport we need. Then merge it with `us_cities_df` by `city_name`. Rank duplicaited city name by abs(delta_latitude) + abs(delta_longitude), assert the number is < 1.5 deg and select the city with smallest error.\n",
    "\n",
    "Generate `city_search_df` for scraping climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df = unpleasant_airport_code_df.merge(airport_loc_df,how='inner',left_index=True,right_index=True)\n",
    "\n",
    "city_search_df = pd.DataFrame(columns=[\"airport_code\",\"state_id\",\"county_id\",\"city_id\",\"fips\",\"population\"])\n",
    "for ind,row in airport_prop_df.iterrows():\n",
    "    city = row[\"city_name\"]\n",
    "    target_lat = row[\"latitude\"]\n",
    "    target_lng = row[\"longitude\"]\n",
    "    try:      \n",
    "        target_cities = us_cities_df[us_cities_df[\"city_name\"]==city]\n",
    "        \n",
    "        if not (target_cities.shape[0] == 1): # If there are multiple city with same name\n",
    "            def calc_dis(input_):\n",
    "                err = abs(target_lat - input_[\"lat\"]) + abs(target_lng - input_[\"lng\"])\n",
    "                return err\n",
    "            target_cities.loc[:,\"error\"] = (target_cities.apply(calc_dis,axis=1))\n",
    "            target_city = target_cities.sort_values(by=\"error\").iloc[0]\n",
    "            \n",
    "            assert target_city[\"error\"] < 1.5 # assert the error should be <1.5 degs ~ 40 miles.\n",
    "            \n",
    "            target_city = target_city.drop([\"error\"])\n",
    "        elif (target_cities.shape[0] >= 1):\n",
    "            target_city = target_cities.iloc[0]\n",
    "        \n",
    "        county = str(target_city[\"county_fips\"])\n",
    "        if (len(county)==1):\n",
    "            county = \"00\" + county\n",
    "        elif (len(county)==2):\n",
    "            county = \"0\" + county\n",
    "\n",
    "        city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":target_city[\"state_id\"],\"county_id\":county,\"city_id\":target_city.name,\"fips\":target_city[\"fips\"],\"population\":target_city[\"population\"]},ignore_index=True)        \n",
    "    except:\n",
    "        try:\n",
    "            def calc_dis(input_):\n",
    "                err = abs(target_lat - input_[\"lat\"]) + abs(target_lng - input_[\"lng\"])\n",
    "                return err\n",
    "            us_cities_df_copy = us_cities_df\n",
    "            us_cities_df_copy.loc[:,\"error\"] = (us_cities_df.apply(calc_dis,axis=1))\n",
    "            target_city = us_cities_df_copy.sort_values(by=\"error\").iloc[0]\n",
    "            assert target_city[\"error\"] < 1.5\n",
    "            county = str(target_city[\"county_fips\"])\n",
    "            if (len(county)==1):\n",
    "                county = \"00\" + county\n",
    "            elif (len(county)==2):\n",
    "                county = \"0\" + county\n",
    "            city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":target_city[\"state_id\"],\"county_id\":county,\"city_id\":target_city.name,\"fips\":target_city[\"fips\"],\"population\":target_city[\"population\"]},ignore_index=True)\n",
    "        except:\n",
    "            print(\"No data for \",city)\n",
    "            city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":np.nan,\"county_id\":np.nan,\"city_id\":np.nan,\"fips\":np.nan,\"population\":np.nan},ignore_index=True)\n",
    "\n",
    "#special case for DC\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    if (row[\"airport_code\"]==\"DCA\"):\n",
    "        city_search_df.iloc[ind][\"state_id\"]=\"MD\"\n",
    "        city_search_df.iloc[ind][\"county_id\"]=\"511\"\n",
    "\n",
    "# Remove Hawaii and Alaska\n",
    "city_search_df = city_search_df[(city_search_df[\"state_id\"]!=\"HI\") & (city_search_df[\"state_id\"]!=\"AK\")]\n",
    "airport_prop_df = airport_prop_df.merge(city_search_df.set_index(\"airport_code\").loc[:,[\"city_id\",\"fips\",\"population\"]],left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Download and process temperature and precipitation data\n",
    "From https://www.ncdc.noaa.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_climate_data(state,county,year):\n",
    "    save_path = \"datasets/original/weather/\"\n",
    "    fname = state + county + \"_\" + str(year) + \".csv\"\n",
    "    if (len(glob.glob(save_path + fname))==0):\n",
    "        URL = \"https://www.ncdc.noaa.gov/cag/county/time-series/{}-{}-{}-all-1-2000-2020.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state,county,\"tavg\")\n",
    "        r = requests.get(URL)\n",
    "        file = wget.download(URL,out=save_path + \"tavg/tavg_\" + fname)\n",
    "        URL = \"https://www.ncdc.noaa.gov/cag/county/time-series/{}-{}-{}-all-1-2000-2020.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state,county,\"pcp\")\n",
    "        r = requests.get(URL)\n",
    "        file = wget.download(URL,out=save_path + \"pcp/pcp_\" + fname)\n",
    "\n",
    "        tavg_df = pd.read_csv(save_path + \"tavg/tavg_\" + fname).iloc[4:]\n",
    "        tavg_df.columns=[\"date\",\"tavg\",\"comp\"]\n",
    "        tavg = tavg_df.set_index(\"date\")[\"tavg\"]\n",
    "\n",
    "        pcp_df = pd.read_csv(save_path + \"pcp/pcp_\" + fname).iloc[4:]\n",
    "        pcp_df.columns=[\"date\",\"pcp\",\"comp\"]\n",
    "        pcp = pcp_df.set_index(\"date\")[\"pcp\"]\n",
    "\n",
    "        pd.concat([tavg, pcp], axis=1).to_csv(save_path + fname)\n",
    "        time.sleep(1) # not requesting too frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_code</th>\n",
       "      <th>state_id</th>\n",
       "      <th>county_id</th>\n",
       "      <th>city_id</th>\n",
       "      <th>fips</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>PA</td>\n",
       "      <td>077</td>\n",
       "      <td>10988</td>\n",
       "      <td>42077</td>\n",
       "      <td>682899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>TX</td>\n",
       "      <td>441</td>\n",
       "      <td>5333</td>\n",
       "      <td>48441</td>\n",
       "      <td>114964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>NM</td>\n",
       "      <td>001</td>\n",
       "      <td>3742</td>\n",
       "      <td>35001</td>\n",
       "      <td>758523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABR</td>\n",
       "      <td>SD</td>\n",
       "      <td>013</td>\n",
       "      <td>28674</td>\n",
       "      <td>46013</td>\n",
       "      <td>28264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>GA</td>\n",
       "      <td>095</td>\n",
       "      <td>17309</td>\n",
       "      <td>13095</td>\n",
       "      <td>90515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_code state_id county_id city_id   fips population\n",
       "0          ABE       PA       077   10988  42077     682899\n",
       "1          ABI       TX       441    5333  48441     114964\n",
       "2          ABQ       NM       001    3742  35001     758523\n",
       "3          ABR       SD       013   28674  46013      28264\n",
       "4          ABY       GA       095   17309  13095      90515"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_search_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.31%   Just done: ABE\r",
      "progress: 0.62%   Just done: ABI\r",
      "progress: 0.93%   Just done: ABQ\r",
      "progress: 1.23%   Just done: ABR\r",
      "progress: 1.54%   Just done: ABY\r",
      "progress: 1.85%   Just done: ACK\r",
      "progress: 2.16%   Just done: ACT\r",
      "progress: 2.47%   Just done: ACV\r",
      "progress: 2.78%   Just done: ACY\r",
      "progress: 3.09%   Just done: AEX\r",
      "progress: 3.40%   Just done: AGS\r",
      "progress: 3.70%   Just done: ALB\r",
      "progress: 4.01%   Just done: ALO\r",
      "progress: 4.32%   Just done: AMA\r",
      "progress: 4.63%   Just done: APN\r",
      "progress: 4.94%   Just done: ART\r",
      "progress: 5.25%   Just done: ASE\r",
      "progress: 5.56%   Just done: ATL\r",
      "progress: 5.86%   Just done: ATW\r",
      "progress: 6.17%   Just done: AUS\r",
      "progress: 6.48%   Just done: AVL\r",
      "progress: 6.79%   Just done: AVP\r",
      "progress: 7.10%   Just done: AZA\r",
      "progress: 7.41%   Just done: AZO\r",
      "progress: 7.72%   Just done: BDL\r",
      "progress: 8.02%   Just done: BFF\r",
      "progress: 8.33%   Just done: BFL\r",
      "progress: 8.64%   Just done: BGM\r",
      "progress: 8.95%   Just done: BGR\r",
      "progress: 9.26%   Just done: BHM\r",
      "progress: 9.57%   Just done: BIL\r",
      "progress: 9.88%   Just done: BIS\r",
      "progress: 10.19%   Just done: BJI\r",
      "progress: 10.49%   Just done: BKG\r",
      "progress: 10.80%   Just done: BLI\r",
      "progress: 11.11%   Just done: BLV\r",
      "progress: 11.42%   Just done: BMI\r",
      "progress: 11.73%   Just done: BNA\r",
      "progress: 12.04%   Just done: BOI\r",
      "progress: 12.35%   Just done: BOS\r",
      "progress: 12.65%   Just done: BPT\r",
      "progress: 12.96%   Just done: BQK\r",
      "progress: 13.27%   Just done: BRD\r",
      "progress: 13.58%   Just done: BRO\r",
      "progress: 13.89%   Just done: BTM\r",
      "progress: 14.20%   Just done: BTR\r",
      "progress: 14.51%   Just done: BTV\r",
      "progress: 14.81%   Just done: BUF\r",
      "progress: 15.12%   Just done: BUR\r",
      "progress: 15.43%   Just done: BWI\r",
      "progress: 15.74%   Just done: BZN\r",
      "progress: 16.05%   Just done: CAE\r",
      "progress: 16.36%   Just done: CAK\r",
      "progress: 16.67%   Just done: CDC\r",
      "progress: 16.98%   Just done: CGI\r",
      "progress: 17.28%   Just done: CHA\r",
      "progress: 17.59%   Just done: CHO\r",
      "progress: 17.90%   Just done: CHS\r",
      "progress: 18.21%   Just done: CID\r",
      "progress: 18.52%   Just done: CIU\r",
      "progress: 18.83%   Just done: CKB\r",
      "progress: 19.14%   Just done: CLE\r",
      "progress: 19.44%   Just done: CLL\r",
      "progress: 19.75%   Just done: CLT\r",
      "progress: 20.06%   Just done: CMH\r",
      "progress: 20.37%   Just done: CMI\r",
      "progress: 20.68%   Just done: CMX\r",
      "progress: 20.99%   Just done: CNY\r",
      "progress: 21.30%   Just done: COD\r",
      "progress: 21.60%   Just done: COS\r",
      "progress: 21.91%   Just done: COU\r",
      "progress: 22.22%   Just done: CPR\r",
      "progress: 22.53%   Just done: CRP\r",
      "progress: 22.84%   Just done: CRW\r",
      "progress: 23.15%   Just done: CSG\r",
      "progress: 23.46%   Just done: CVG\r",
      "progress: 23.77%   Just done: CWA\r",
      "progress: 24.07%   Just done: CYS\r",
      "progress: 24.38%   Just done: DAB\r",
      "progress: 24.69%   Just done: DAL\r",
      "progress: 25.00%   Just done: DAY\r",
      "progress: 25.31%   Just done: DBQ\r",
      "progress: 25.62%   Just done: DCA\r",
      "progress: 25.93%   Just done: DEN\r",
      "progress: 26.23%   Just done: DFW\r",
      "progress: 26.54%   Just done: DHN\r",
      "progress: 26.85%   Just done: DLH\r",
      "progress: 27.16%   Just done: DRO\r",
      "progress: 27.47%   Just done: DRT\r",
      "progress: 27.78%   Just done: DSM\r",
      "progress: 28.09%   Just done: DTW\r",
      "progress: 28.40%   Just done: DVL\r",
      "progress: 28.70%   Just done: EAR\r",
      "progress: 29.01%   Just done: EAU\r",
      "progress: 29.32%   Just done: ECP\r",
      "progress: 29.63%   Just done: EGE\r",
      "progress: 29.94%   Just done: EKO\r",
      "progress: 30.25%   Just done: ELM\r",
      "progress: 30.56%   Just done: ELP\r",
      "progress: 30.86%   Just done: ERI\r",
      "progress: 31.17%   Just done: ESC\r",
      "progress: 31.48%   Just done: EUG\r",
      "progress: 31.79%   Just done: EVV\r",
      "progress: 32.10%   Just done: EWN\r",
      "progress: 32.41%   Just done: EWR\r",
      "progress: 32.72%   Just done: EYW\r",
      "progress: 33.02%   Just done: FAR\r",
      "progress: 33.33%   Just done: FAT\r",
      "progress: 33.64%   Just done: FAY\r",
      "progress: 33.95%   Just done: FCA\r",
      "progress: 34.26%   Just done: FLG\r",
      "progress: 34.57%   Just done: FLL\r",
      "progress: 34.88%   Just done: FLO\r",
      "progress: 35.19%   Just done: FNT\r",
      "progress: 35.49%   Just done: FSD\r",
      "progress: 35.80%   Just done: FSM\r",
      "progress: 36.11%   Just done: FWA\r",
      "progress: 36.42%   Just done: GCC\r",
      "progress: 36.73%   Just done: GCK\r",
      "progress: 37.04%   Just done: GEG\r",
      "progress: 37.35%   Just done: GFK\r",
      "progress: 37.65%   Just done: GGG\r",
      "progress: 37.96%   Just done: GJT\r",
      "progress: 38.27%   Just done: GNV\r",
      "progress: 38.58%   Just done: GPT\r",
      "progress: 38.89%   Just done: GRB\r",
      "progress: 39.20%   Just done: GRI\r",
      "progress: 39.51%   Just done: GRK\r",
      "progress: 39.81%   Just done: GRR\r",
      "progress: 40.12%   Just done: GSO\r",
      "progress: 40.43%   Just done: GSP\r",
      "progress: 40.74%   Just done: GTF\r",
      "progress: 41.05%   Just done: GTR\r",
      "progress: 41.36%   Just done: GUC\r",
      "progress: 41.67%   Just done: HDN\r",
      "progress: 41.98%   Just done: HGR\r",
      "progress: 42.28%   Just done: HHH\r",
      "progress: 42.59%   Just done: HIB\r",
      "progress: 42.90%   Just done: HLN\r",
      "progress: 43.21%   Just done: HOB\r",
      "progress: 43.52%   Just done: HOU\r",
      "progress: 43.83%   Just done: HPN\r",
      "progress: 44.14%   Just done: HRL\r",
      "progress: 44.44%   Just done: HSV\r",
      "progress: 44.75%   Just done: HTS\r",
      "progress: 45.06%   Just done: HVN\r",
      "progress: 45.37%   Just done: HYA\r",
      "progress: 45.68%   Just done: HYS\r",
      "progress: 45.99%   Just done: IAD\r",
      "progress: 46.30%   Just done: IAG\r",
      "progress: 46.60%   Just done: IAH\r",
      "progress: 46.91%   Just done: ICT\r",
      "progress: 47.22%   Just done: IDA\r",
      "progress: 47.53%   Just done: IFP\r",
      "progress: 47.84%   Just done: ILM\r",
      "progress: 48.15%   Just done: IMT\r",
      "progress: 48.46%   Just done: IND\r",
      "progress: 48.77%   Just done: INL\r",
      "progress: 49.07%   Just done: ISN\r",
      "progress: 49.38%   Just done: ISP\r",
      "progress: 49.69%   Just done: ITH\r",
      "progress: 50.00%   Just done: JAC\r",
      "progress: 50.31%   Just done: JAN\r",
      "progress: 50.62%   Just done: JAX\r",
      "progress: 50.93%   Just done: JFK\r",
      "progress: 51.23%   Just done: JLN\r",
      "progress: 51.54%   Just done: JMS\r",
      "progress: 51.85%   Just done: LAN\r",
      "progress: 52.16%   Just done: LAR\r",
      "progress: 52.47%   Just done: LAS\r",
      "progress: 52.78%   Just done: LAW\r",
      "progress: 53.09%   Just done: LAX\r",
      "progress: 53.40%   Just done: LBB\r",
      "progress: 53.70%   Just done: LBE\r",
      "progress: 54.01%   Just done: LBF\r",
      "progress: 54.32%   Just done: LBL\r",
      "progress: 54.63%   Just done: LCH\r",
      "progress: 54.94%   Just done: LCK\r",
      "progress: 55.25%   Just done: LEX\r",
      "progress: 55.56%   Just done: LFT\r",
      "progress: 55.86%   Just done: LGA\r",
      "progress: 56.17%   Just done: LGB\r",
      "progress: 56.48%   Just done: LIT\r",
      "progress: 56.79%   Just done: LNK\r",
      "progress: 57.10%   Just done: LRD\r",
      "progress: 57.41%   Just done: LSE\r",
      "progress: 57.72%   Just done: LWB\r",
      "progress: 58.02%   Just done: LWS\r",
      "progress: 58.33%   Just done: LYH\r",
      "progress: 58.64%   Just done: MAF\r",
      "progress: 58.95%   Just done: MBS\r",
      "progress: 59.26%   Just done: MCI\r",
      "progress: 59.57%   Just done: MCO\r",
      "progress: 59.88%   Just done: MDT\r",
      "progress: 60.19%   Just done: MDW\r",
      "progress: 60.49%   Just done: MEI\r",
      "progress: 60.80%   Just done: MEM\r",
      "progress: 61.11%   Just done: MFE\r",
      "progress: 61.42%   Just done: MFR\r",
      "progress: 61.73%   Just done: MGM\r",
      "progress: 62.04%   Just done: MHK\r",
      "progress: 62.35%   Just done: MHT\r",
      "progress: 62.65%   Just done: MIA\r",
      "progress: 62.96%   Just done: MKE\r",
      "progress: 63.27%   Just done: MKG\r",
      "progress: 63.58%   Just done: MLB\r",
      "progress: 63.89%   Just done: MLI\r",
      "progress: 64.20%   Just done: MLU\r",
      "progress: 64.51%   Just done: MMH\r",
      "progress: 64.81%   Just done: MOB\r",
      "progress: 65.12%   Just done: MOT\r",
      "progress: 65.43%   Just done: MQT\r",
      "progress: 65.74%   Just done: MRY\r",
      "progress: 66.05%   Just done: MSN\r",
      "progress: 66.36%   Just done: MSO\r",
      "progress: 66.67%   Just done: MSP\r",
      "progress: 66.98%   Just done: MSY\r",
      "progress: 67.28%   Just done: MTJ\r",
      "progress: 67.59%   Just done: MVY\r",
      "progress: 67.90%   Just done: MYR\r",
      "progress: 68.21%   Just done: OAJ\r",
      "progress: 68.52%   Just done: OAK\r",
      "progress: 68.83%   Just done: OGD\r",
      "progress: 69.14%   Just done: OGS\r",
      "progress: 69.44%   Just done: OKC\r",
      "progress: 69.75%   Just done: OMA\r",
      "progress: 70.06%   Just done: ONT\r",
      "progress: 70.37%   Just done: ORD\r",
      "progress: 70.68%   Just done: ORF\r",
      "progress: 70.99%   Just done: ORH\r",
      "progress: 71.30%   Just done: OTH\r",
      "progress: 71.60%   Just done: OWB\r",
      "progress: 71.91%   Just done: PAH\r",
      "progress: 72.22%   Just done: PBG\r",
      "progress: 72.53%   Just done: PBI\r",
      "progress: 72.84%   Just done: PDX\r",
      "progress: 73.15%   Just done: PGD\r",
      "progress: 73.46%   Just done: PGV\r",
      "progress: 73.77%   Just done: PHF\r",
      "progress: 74.07%   Just done: PHL\r",
      "progress: 74.38%   Just done: PHX\r",
      "progress: 74.69%   Just done: PIA\r",
      "progress: 75.00%   Just done: PIB\r",
      "progress: 75.31%   Just done: PIE\r",
      "progress: 75.62%   Just done: PIH\r",
      "progress: 75.93%   Just done: PIT\r",
      "progress: 76.23%   Just done: PLN\r",
      "progress: 76.54%   Just done: PNS\r",
      "progress: 76.85%   Just done: PRC\r",
      "progress: 77.16%   Just done: PSC\r",
      "progress: 77.47%   Just done: PSM\r",
      "progress: 77.78%   Just done: PSP\r",
      "progress: 78.09%   Just done: PUB\r",
      "progress: 78.40%   Just done: PVD\r",
      "progress: 78.70%   Just done: PVU\r",
      "progress: 79.01%   Just done: PWM\r",
      "progress: 79.32%   Just done: RAP\r",
      "progress: 79.63%   Just done: RDD\r",
      "progress: 79.94%   Just done: RDM\r",
      "progress: 80.25%   Just done: RDU\r",
      "progress: 80.56%   Just done: RFD\r",
      "progress: 80.86%   Just done: RHI\r",
      "progress: 81.17%   Just done: RIC\r",
      "progress: 81.48%   Just done: RKS\r",
      "progress: 81.79%   Just done: RNO\r",
      "progress: 82.10%   Just done: ROA\r",
      "progress: 82.41%   Just done: ROC\r",
      "progress: 82.72%   Just done: ROW\r",
      "progress: 83.02%   Just done: RST\r",
      "progress: 83.33%   Just done: RSW\r",
      "progress: 83.64%   Just done: SAF\r",
      "progress: 83.95%   Just done: SAN\r",
      "progress: 84.26%   Just done: SAT\r",
      "progress: 84.57%   Just done: SAV\r",
      "progress: 84.88%   Just done: SBA\r",
      "progress: 85.19%   Just done: SBN\r",
      "progress: 85.49%   Just done: SBP\r",
      "progress: 85.80%   Just done: SCE\r",
      "progress: 86.11%   Just done: SCK\r",
      "progress: 86.42%   Just done: SDF\r",
      "progress: 86.73%   Just done: SEA\r",
      "progress: 87.04%   Just done: SFB\r",
      "progress: 87.35%   Just done: SFO\r",
      "progress: 87.65%   Just done: SGF\r",
      "progress: 87.96%   Just done: SGU\r",
      "progress: 88.27%   Just done: SHD\r",
      "progress: 88.58%   Just done: SHV\r",
      "progress: 88.89%   Just done: SJC\r",
      "progress: 89.20%   Just done: SJT\r",
      "progress: 89.51%   Just done: SLC\r",
      "progress: 89.81%   Just done: SLN\r",
      "progress: 90.12%   Just done: SMF\r",
      "progress: 90.43%   Just done: SMX\r",
      "progress: 90.74%   Just done: SNA\r",
      "progress: 91.05%   Just done: SPI\r",
      "progress: 91.36%   Just done: SPS\r",
      "progress: 91.67%   Just done: SRQ\r",
      "progress: 91.98%   Just done: STC\r",
      "progress: 92.28%   Just done: STL\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 92.59%   Just done: STS\r",
      "progress: 92.90%   Just done: SUN\r",
      "progress: 93.21%   Just done: SUX\r",
      "progress: 93.52%   Just done: SWF\r",
      "progress: 93.83%   Just done: SYR\r",
      "progress: 94.14%   Just done: TLH\r",
      "progress: 94.44%   Just done: TOL\r",
      "progress: 94.75%   Just done: TPA\r",
      "progress: 95.06%   Just done: TRI\r",
      "progress: 95.37%   Just done: TTN\r",
      "progress: 95.68%   Just done: TUL\r",
      "progress: 95.99%   Just done: TUS\r",
      "progress: 96.30%   Just done: TVC\r",
      "progress: 96.60%   Just done: TWF\r",
      "progress: 96.91%   Just done: TXK\r",
      "progress: 97.22%   Just done: TYR\r",
      "progress: 97.53%   Just done: TYS\r",
      "progress: 97.84%   Just done: UIN\r",
      "progress: 98.15%   Just done: VEL\r",
      "progress: 98.46%   Just done: VLD\r",
      "progress: 98.77%   Just done: VPS\r",
      "progress: 99.07%   Just done: WYS\r",
      "progress: 99.38%   Just done: XNA\r",
      "progress: 99.69%   Just done: YNG\r",
      "progress: 100.00%   Just done: YUM\r"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "# Download data according to city_search_df\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    try:\n",
    "        download_climate_data(row[\"state_id\"],row[\"county_id\"],2018)\n",
    "        download_climate_data(row[\"state_id\"],row[\"county_id\"],2019)\n",
    "    except:\n",
    "        print(row)\n",
    "    counter+=1\n",
    "    print(\"progress: {:.2f}%   Just done: {}\".format(100 * counter / city_search_df.shape[0],row[\"airport_code\"]),end=\"\\r\")\n",
    "\n",
    "# Process downloaded data\n",
    "temp_pcp_df = pd.DataFrame(columns=[\"airport_code\",\"temp_avg\",\"pcp_avg\"])\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    state = row[\"state_id\"]\n",
    "    county = row[\"county_id\"]\n",
    "    save_path = \"datasets/original/weather/\"\n",
    "    years = [2018,2019]\n",
    "    \n",
    "    try:\n",
    "        tavg = 0\n",
    "        pcp = 0\n",
    "        for year in years:\n",
    "            fname = state + county + \"_\" + str(year) + \".csv\"\n",
    "            temp_pcp = pd.read_csv(save_path + fname)\n",
    "            tavg += temp_pcp.mean()[\"tavg\"]\n",
    "            pcp += temp_pcp.mean()[\"pcp\"]\n",
    "        tavg /= len(years)\n",
    "        pcp /= len(years)\n",
    "        \n",
    "        temp_pcp_df = temp_pcp_df.append({\"airport_code\":row[\"airport_code\"],\"temp_avg\":tavg,\"pcp_avg\":pcp},ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        temp_pcp_df = temp_pcp_df.append({\"airport_code\":row[\"airport_code\"],\"temp_avg\":np.nan,\"pcp_avg\":np.nan},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>code4</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>altitude_ft</th>\n",
       "      <th>city_id</th>\n",
       "      <th>fips</th>\n",
       "      <th>population</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>pcp_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airport_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABE</th>\n",
       "      <td>allentown</td>\n",
       "      <td>KABE</td>\n",
       "      <td>40.652100</td>\n",
       "      <td>-75.440804</td>\n",
       "      <td>393</td>\n",
       "      <td>10988</td>\n",
       "      <td>42077</td>\n",
       "      <td>682899</td>\n",
       "      <td>51.902490</td>\n",
       "      <td>4.161037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABI</th>\n",
       "      <td>abilene</td>\n",
       "      <td>KABI</td>\n",
       "      <td>32.411301</td>\n",
       "      <td>-99.681900</td>\n",
       "      <td>1791</td>\n",
       "      <td>5333</td>\n",
       "      <td>48441</td>\n",
       "      <td>114964</td>\n",
       "      <td>64.488797</td>\n",
       "      <td>2.131411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABQ</th>\n",
       "      <td>albuquerque</td>\n",
       "      <td>KABQ</td>\n",
       "      <td>35.040199</td>\n",
       "      <td>-106.609001</td>\n",
       "      <td>5355</td>\n",
       "      <td>3742</td>\n",
       "      <td>35001</td>\n",
       "      <td>758523</td>\n",
       "      <td>54.316598</td>\n",
       "      <td>0.928465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABR</th>\n",
       "      <td>aberdeen</td>\n",
       "      <td>KABR</td>\n",
       "      <td>45.449100</td>\n",
       "      <td>-98.421799</td>\n",
       "      <td>1302</td>\n",
       "      <td>28674</td>\n",
       "      <td>46013</td>\n",
       "      <td>28264</td>\n",
       "      <td>43.304149</td>\n",
       "      <td>1.796141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABY</th>\n",
       "      <td>albany</td>\n",
       "      <td>KABY</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>-84.194504</td>\n",
       "      <td>197</td>\n",
       "      <td>17309</td>\n",
       "      <td>13095</td>\n",
       "      <td>90515</td>\n",
       "      <td>66.578008</td>\n",
       "      <td>4.069336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city_name code4   latitude   longitude  altitude_ft city_id  \\\n",
       "airport_code                                                                  \n",
       "ABE             allentown  KABE  40.652100  -75.440804          393   10988   \n",
       "ABI               abilene  KABI  32.411301  -99.681900         1791    5333   \n",
       "ABQ           albuquerque  KABQ  35.040199 -106.609001         5355    3742   \n",
       "ABR              aberdeen  KABR  45.449100  -98.421799         1302   28674   \n",
       "ABY                albany  KABY  31.535500  -84.194504          197   17309   \n",
       "\n",
       "               fips population   temp_avg   pcp_avg  \n",
       "airport_code                                         \n",
       "ABE           42077     682899  51.902490  4.161037  \n",
       "ABI           48441     114964  64.488797  2.131411  \n",
       "ABQ           35001     758523  54.316598  0.928465  \n",
       "ABR           46013      28264  43.304149  1.796141  \n",
       "ABY           13095      90515  66.578008  4.069336  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_prop_df = airport_prop_df.merge(temp_pcp_df.set_index(\"airport_code\"),left_index=True,right_index=True)\n",
    "airport_prop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Bird Strike dataset - strike_average, damage_average\n",
    "Clean `datasets/original/airports/Bird Strikes.xlsx`. Merge `bird_strike_avg_df` and `unpleasant_airport_code_df` to get `bird_strike_final_df`. Export it as `bird_strike.csv`.There's no airport code in `Bird Strikes.xlsx`, so additional dataset that contains airport name and airport code is used to link them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_name</th>\n",
       "      <th>bird_strike_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAGUARDIA NY</td>\n",
       "      <td>Caused damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DALLAS/FORT WORTH INTL ARPT</td>\n",
       "      <td>Caused damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LAKEFRONT AIRPORT</td>\n",
       "      <td>No damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEATTLE-TACOMA INTL</td>\n",
       "      <td>No damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NORFOLK INTL</td>\n",
       "      <td>No damage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  airport_name bird_strike_effect\n",
       "0                 LAGUARDIA NY      Caused damage\n",
       "1  DALLAS/FORT WORTH INTL ARPT      Caused damage\n",
       "2            LAKEFRONT AIRPORT          No damage\n",
       "3          SEATTLE-TACOMA INTL          No damage\n",
       "4                 NORFOLK INTL          No damage"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_strike_df = pd.read_excel(\"datasets/original/airport/Bird Strikes.xlsx\") #data6\n",
    "airport_name_df = pd.read_excel(\"datasets/original/airport/airportcode.xlsx\") #data7 \n",
    "bird_strike_df = bird_strike_df[[\"Airport: Name\", \"Effect: Indicated Damage\"]]\n",
    "bird_strike_df = bird_strike_df.rename(columns = {\"Airport: Name\": \"airport_name\", \"Effect: Indicated Damage\":\"bird_strike_effect\"})\n",
    "bird_strike_df = bird_strike_df.dropna()\n",
    "bird_strike_df = bird_strike_df.reset_index(drop = True)\n",
    "bird_strike_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_code</th>\n",
       "      <th>airport_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>Lehigh Valley International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>Abilene Regional Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Southwest Georgia Regional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACK</td>\n",
       "      <td>Nantucket Memorial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_code                       airport_name\n",
       "0          ABE        Lehigh Valley International\n",
       "1          ABI           Abilene Regional Airport\n",
       "2          ABQ  Albuquerque International Sunport\n",
       "3          ABY         Southwest Georgia Regional\n",
       "4          ACK                 Nantucket Memorial"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_name_df = airport_name_df.dropna()\n",
    "airport_name_df = airport_name_df.reset_index(drop = True)\n",
    "airport_name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_airport_name(string):\n",
    "    \n",
    "    string = string.lower()\n",
    "    string = string.strip()\n",
    "    if 'intl' in string:\n",
    "        string = string.replace('intl', '')\n",
    "    if 'arpt' in string:\n",
    "        string = string.replace('arpt', '')\n",
    "    if 'regional' in string:\n",
    "        string = string.replace('regional', '')\n",
    "    if 'airport' in string:\n",
    "        string = string.replace('airport', '')\n",
    "    if 'sunport' in string:\n",
    "        string = string.replace('sunport', '')\n",
    "    if 'international' in string:\n",
    "        string = string.replace('international', '')\n",
    "    if 'intercontinental' in string:\n",
    "        string = string.replace('intercontinental', '')\n",
    "    else:\n",
    "        output = string\n",
    "        \n",
    "    string = string = string.strip()\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_name</th>\n",
       "      <th>bird_strike_effect</th>\n",
       "      <th>strike</th>\n",
       "      <th>damage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>laguardia ny</td>\n",
       "      <td>Caused damage</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dallas/fort worth</td>\n",
       "      <td>Caused damage</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lakefront</td>\n",
       "      <td>No damage</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seattle-tacoma</td>\n",
       "      <td>No damage</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>norfolk</td>\n",
       "      <td>No damage</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        airport_name bird_strike_effect  strike  damage\n",
       "0       laguardia ny      Caused damage       1       1\n",
       "1  dallas/fort worth      Caused damage       1       1\n",
       "2          lakefront          No damage       1       0\n",
       "3     seattle-tacoma          No damage       1       0\n",
       "4            norfolk          No damage       1       0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_strike_df['airport_name'] = bird_strike_df['airport_name'].apply(standardize_airport_name)\n",
    "airport_name_df['airport_name'] = airport_name_df['airport_name'].apply(standardize_airport_name)\n",
    "\n",
    "def check_strike (string):\n",
    "    return 1\n",
    "\n",
    "def check_damage (string):\n",
    "    if 'Caused' in string:\n",
    "        output = 1\n",
    "    else:\n",
    "        output = 0\n",
    "    return output\n",
    "\n",
    "bird_strike_df['strike'] = bird_strike_df['bird_strike_effect'].apply(check_strike)\n",
    "bird_strike_df['damage'] = bird_strike_df['bird_strike_effect'].apply(check_damage)\n",
    "bird_strike_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_code</th>\n",
       "      <th>strike_avg</th>\n",
       "      <th>damage_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABY</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACK</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_code  strike_avg  damage_avg\n",
       "0          ABE    5.250000    0.166667\n",
       "1          ABI    0.750000    0.083333\n",
       "2          ABQ    6.833333    0.250000\n",
       "3          ABY    0.416667    0.083333\n",
       "4          ACK    0.583333    0.083333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_strike = bird_strike_df.groupby('airport_name').agg({'strike':['sum']})\n",
    "grouped_strike = grouped_strike.reset_index()\n",
    "grouped_damage = bird_strike_df.groupby('airport_name').agg({'damage':['sum']})\n",
    "\n",
    "bird_strike_sum_df = pd.merge(grouped_strike, grouped_damage, on='airport_name')\n",
    "bird_strike_sum_df.columns = ['airport_name', 'strike_sum','damage_sum']\n",
    "\n",
    "bird_strike_avg_df = pd.merge(airport_name_df, bird_strike_sum_df, on='airport_name')\n",
    "\n",
    "def average_sum(input):\n",
    "    output = input/(2011 - 2000 + 1)\n",
    "    return output\n",
    "\n",
    "bird_strike_avg_df['strike_avg'] = bird_strike_avg_df['strike_sum'].apply(average_sum)\n",
    "bird_strike_avg_df['damage_avg'] = bird_strike_avg_df['damage_sum'].apply(average_sum)\n",
    "bird_strike_avg_df = bird_strike_avg_df.drop(columns = ['strike_sum', 'damage_sum'])\n",
    "\n",
    "bird_strike_avg_df= bird_strike_avg_df.drop(columns = [\"airport_name\"])\n",
    "bird_strike_avg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We later need to merge this dataframe to `airport_prop_df`. However, not all airports have bird strike. Therefore, we used a right merge(that kept all rows from `airport_prop_df`) and filled nan with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strike_avg</th>\n",
       "      <th>damage_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airport_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABE</th>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABI</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABQ</th>\n",
       "      <td>6.833333</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABY</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACK</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              strike_avg  damage_avg\n",
       "airport_code                        \n",
       "ABE             5.250000    0.166667\n",
       "ABI             0.750000    0.083333\n",
       "ABQ             6.833333    0.250000\n",
       "ABY             0.416667    0.083333\n",
       "ACK             0.583333    0.083333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_strike_final_df = pd.merge(bird_strike_avg_df, unpleasant_airport_code_df.reset_index(), how='right')\n",
    "# unpleasant_airport_code_df have the same index as airport_prop_df\n",
    "bird_strike_final_df = bird_strike_final_df.fillna(0)\n",
    "bird_strike_final_df = bird_strike_final_df.set_index(\"airport_code\")\n",
    "bird_strike_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Runways dataset - runway length, width, count\n",
    "Clean `datasets/original/airport/runways.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code4</th>\n",
       "      <th>airport_code</th>\n",
       "      <th>length_ft</th>\n",
       "      <th>width_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07FA</td>\n",
       "      <td>OCA</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5A8</td>\n",
       "      <td>WKK</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HI07</td>\n",
       "      <td>WKL</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K0V4</td>\n",
       "      <td>0V4</td>\n",
       "      <td>3798.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K19S</td>\n",
       "      <td>19S</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code4 airport_code  length_ft  width_ft\n",
       "0  07FA          OCA     4500.0      70.0\n",
       "1   5A8          WKK     2040.0      80.0\n",
       "2  HI07          WKL       26.0      26.0\n",
       "3  K0V4          0V4     3798.0      60.0\n",
       "4  K19S          19S     2300.0     100.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_runways_df=pd.read_csv(\"datasets/original/airport/runways.csv\")\n",
    "airport_runways_df = airport_runways_df.rename(columns = {\"airport_ident\":\"code4\"})\n",
    "airport_runways_df = pd.merge(airport_runways_df, airport_loc_df.reset_index(), how = 'inner', on = 'code4')\n",
    "airport_runways_df = airport_runways_df[[\"code4\", \"airport_code\", \"length_ft\", \"width_ft\"]]\n",
    "airport_runways_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_ft_sum</th>\n",
       "      <th>width_ft_avg</th>\n",
       "      <th>runway_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airport_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0V4</th>\n",
       "      <td>3798.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19S</th>\n",
       "      <td>6800.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23M</th>\n",
       "      <td>3200.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2A5</th>\n",
       "      <td>3800.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2H0</th>\n",
       "      <td>9948.0</td>\n",
       "      <td>158.333333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              length_ft_sum  width_ft_avg  runway_count\n",
       "airport_code                                           \n",
       "0V4                  3798.0     60.000000             1\n",
       "19S                  6800.0     80.000000             2\n",
       "23M                  3200.0     60.000000             1\n",
       "2A5                  3800.0     40.000000             1\n",
       "2H0                  9948.0    158.333333             3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runways_sum_df = airport_runways_df.groupby(['airport_code']).sum()\n",
    "runways_mean_df = airport_runways_df.groupby(['airport_code']).mean()\n",
    "runways_count_df = airport_runways_df.groupby(['airport_code']).count()\n",
    "runway_final_df = runways_sum_df[\"length_ft\"].to_frame().join(runways_mean_df[\"width_ft\"].to_frame())\n",
    "runway_final_df[\"count\"] = runways_count_df[\"length_ft\"]\n",
    "runway_final_df.columns = [\"length_ft_sum\", \"width_ft_avg\",\"runway_count\"]\n",
    "\n",
    "runway_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 enplanements dataset - enplanements\n",
    "Clean `commercial_service_enplanements.xlsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enplanements</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airport_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ATL</th>\n",
       "      <td>76184862.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAX</th>\n",
       "      <td>62544457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORD</th>\n",
       "      <td>58529991.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DFW</th>\n",
       "      <td>48227832.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEN</th>\n",
       "      <td>45490567.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              enplanements\n",
       "airport_code              \n",
       "ATL             76184862.5\n",
       "LAX             62544457.0\n",
       "ORD             58529991.5\n",
       "DFW             48227832.5\n",
       "DEN             45490567.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enplanements_df = pd.read_excel('datasets/original/city/commercial_service_enplanements.xlsx')\n",
    "enplanements_df = enplanements_df.rename(columns = {\"Locid\":\"airport_code\", \"% Change\":\"enplanement_change\", \"CY 18 Enplanements\":\"enplanements_18\", \"CY 17 Enplanements\":\"enplanements_17\"})\n",
    "enplanements_df = enplanements_df.set_index(\"airport_code\")\n",
    "enplanements_df = enplanements_df[[\"enplanements_17\", \"enplanements_18\"]]\n",
    "\n",
    "temp_enplanements = enplanements_df[\"enplanements_17\"]+enplanements_df[\"enplanements_18\"]*0.5\n",
    "enplanements_df[\"enplanements\"] = temp_enplanements\n",
    "enplanements_df = enplanements_df[[\"enplanements\"]]\n",
    "enplanements_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Cleaning step: merging all dataframes together to get X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>code4</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>altitude_ft</th>\n",
       "      <th>city_id</th>\n",
       "      <th>fips</th>\n",
       "      <th>population</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>pcp_avg</th>\n",
       "      <th>strike_avg</th>\n",
       "      <th>damage_avg</th>\n",
       "      <th>enplanements</th>\n",
       "      <th>length_ft_sum</th>\n",
       "      <th>width_ft_avg</th>\n",
       "      <th>runway_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABE</th>\n",
       "      <td>allentown</td>\n",
       "      <td>KABE</td>\n",
       "      <td>40.652100</td>\n",
       "      <td>-75.440804</td>\n",
       "      <td>393.0</td>\n",
       "      <td>10988</td>\n",
       "      <td>42077</td>\n",
       "      <td>682899</td>\n",
       "      <td>51.902490</td>\n",
       "      <td>4.161037</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>517148.0</td>\n",
       "      <td>13397.0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABI</th>\n",
       "      <td>abilene</td>\n",
       "      <td>KABI</td>\n",
       "      <td>32.411301</td>\n",
       "      <td>-99.681900</td>\n",
       "      <td>1791.0</td>\n",
       "      <td>5333</td>\n",
       "      <td>48441</td>\n",
       "      <td>114964</td>\n",
       "      <td>64.488797</td>\n",
       "      <td>2.131411</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>123699.5</td>\n",
       "      <td>18078.0</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABQ</th>\n",
       "      <td>albuquerque</td>\n",
       "      <td>KABQ</td>\n",
       "      <td>35.040199</td>\n",
       "      <td>-106.609001</td>\n",
       "      <td>5355.0</td>\n",
       "      <td>3742</td>\n",
       "      <td>35001</td>\n",
       "      <td>758523</td>\n",
       "      <td>54.316598</td>\n",
       "      <td>0.928465</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3735962.5</td>\n",
       "      <td>39793.0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABR</th>\n",
       "      <td>aberdeen</td>\n",
       "      <td>KABR</td>\n",
       "      <td>45.449100</td>\n",
       "      <td>-98.421799</td>\n",
       "      <td>1302.0</td>\n",
       "      <td>28674</td>\n",
       "      <td>46013</td>\n",
       "      <td>28264</td>\n",
       "      <td>43.304149</td>\n",
       "      <td>1.796141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41837.0</td>\n",
       "      <td>12401.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABY</th>\n",
       "      <td>albany</td>\n",
       "      <td>KABY</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>-84.194504</td>\n",
       "      <td>197.0</td>\n",
       "      <td>17309</td>\n",
       "      <td>13095</td>\n",
       "      <td>90515</td>\n",
       "      <td>66.578008</td>\n",
       "      <td>4.069336</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>57631.0</td>\n",
       "      <td>11801.0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VLD</th>\n",
       "      <td>valdosta</td>\n",
       "      <td>KVLD</td>\n",
       "      <td>30.782499</td>\n",
       "      <td>-83.276703</td>\n",
       "      <td>203.0</td>\n",
       "      <td>17227</td>\n",
       "      <td>13185</td>\n",
       "      <td>79294</td>\n",
       "      <td>66.983817</td>\n",
       "      <td>4.138382</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>66042.5</td>\n",
       "      <td>17236.0</td>\n",
       "      <td>108.333333</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPS</th>\n",
       "      <td>valparaiso</td>\n",
       "      <td>KVPS</td>\n",
       "      <td>30.483200</td>\n",
       "      <td>-86.525398</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2510</td>\n",
       "      <td>12091</td>\n",
       "      <td>4946</td>\n",
       "      <td>67.168050</td>\n",
       "      <td>5.020456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>909512.5</td>\n",
       "      <td>22017.0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WYS</th>\n",
       "      <td>yellowstone</td>\n",
       "      <td>KWYS</td>\n",
       "      <td>44.688400</td>\n",
       "      <td>-111.117996</td>\n",
       "      <td>6649.0</td>\n",
       "      <td>26356</td>\n",
       "      <td>30031</td>\n",
       "      <td>1365</td>\n",
       "      <td>40.186307</td>\n",
       "      <td>2.004440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12886.0</td>\n",
       "      <td>8399.0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XNA</th>\n",
       "      <td>bentonville</td>\n",
       "      <td>KXNA</td>\n",
       "      <td>36.281898</td>\n",
       "      <td>-94.306801</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>15451</td>\n",
       "      <td>5007</td>\n",
       "      <td>49298</td>\n",
       "      <td>58.123651</td>\n",
       "      <td>3.907842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1077820.5</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YNG</th>\n",
       "      <td>youngstown</td>\n",
       "      <td>KYNG</td>\n",
       "      <td>41.260700</td>\n",
       "      <td>-80.679100</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>21828</td>\n",
       "      <td>39099</td>\n",
       "      <td>373728</td>\n",
       "      <td>50.026971</td>\n",
       "      <td>3.509461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37773.0</td>\n",
       "      <td>17505.0</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       city_name code4   latitude   longitude  altitude_ft city_id   fips  \\\n",
       "ABE    allentown  KABE  40.652100  -75.440804        393.0   10988  42077   \n",
       "ABI      abilene  KABI  32.411301  -99.681900       1791.0    5333  48441   \n",
       "ABQ  albuquerque  KABQ  35.040199 -106.609001       5355.0    3742  35001   \n",
       "ABR     aberdeen  KABR  45.449100  -98.421799       1302.0   28674  46013   \n",
       "ABY       albany  KABY  31.535500  -84.194504        197.0   17309  13095   \n",
       "..           ...   ...        ...         ...          ...     ...    ...   \n",
       "VLD     valdosta  KVLD  30.782499  -83.276703        203.0   17227  13185   \n",
       "VPS   valparaiso  KVPS  30.483200  -86.525398         87.0    2510  12091   \n",
       "WYS  yellowstone  KWYS  44.688400 -111.117996       6649.0   26356  30031   \n",
       "XNA  bentonville  KXNA  36.281898  -94.306801       1287.0   15451   5007   \n",
       "YNG   youngstown  KYNG  41.260700  -80.679100       1192.0   21828  39099   \n",
       "\n",
       "    population   temp_avg   pcp_avg  strike_avg  damage_avg  enplanements  \\\n",
       "ABE     682899  51.902490  4.161037    5.250000    0.166667      517148.0   \n",
       "ABI     114964  64.488797  2.131411    0.750000    0.083333      123699.5   \n",
       "ABQ     758523  54.316598  0.928465    6.833333    0.250000     3735962.5   \n",
       "ABR      28264  43.304149  1.796141    0.000000    0.000000       41837.0   \n",
       "ABY      90515  66.578008  4.069336    0.416667    0.083333       57631.0   \n",
       "..         ...        ...       ...         ...         ...           ...   \n",
       "VLD      79294  66.983817  4.138382    0.250000    0.166667       66042.5   \n",
       "VPS       4946  67.168050  5.020456    0.000000    0.000000      909512.5   \n",
       "WYS       1365  40.186307  2.004440    0.000000    0.000000       12886.0   \n",
       "XNA      49298  58.123651  3.907842    0.000000    0.000000     1077820.5   \n",
       "YNG     373728  50.026971  3.509461    0.000000    0.000000       37773.0   \n",
       "\n",
       "     length_ft_sum  width_ft_avg  runway_count  \n",
       "ABE        13397.0    150.000000           2.0  \n",
       "ABI        18078.0    133.333333           3.0  \n",
       "ABQ        39793.0    150.000000           4.0  \n",
       "ABR        12401.0    100.000000           2.0  \n",
       "ABY        11801.0    150.000000           2.0  \n",
       "..             ...           ...           ...  \n",
       "VLD        17236.0    108.333333           3.0  \n",
       "VPS        22017.0    300.000000           2.0  \n",
       "WYS         8399.0    150.000000           1.0  \n",
       "XNA         8800.0    150.000000           1.0  \n",
       "YNG        17505.0    120.000000           3.0  \n",
       "\n",
       "[313 rows x 16 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_X_df = pd.concat([airport_prop_df, bird_strike_final_df, enplanements_df, runway_final_df], axis=1, sort=False)\n",
    "merged_X_df = merged_X_df.dropna()\n",
    "merged_X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_departure_delay</th>\n",
       "      <th>average_arrival_delay</th>\n",
       "      <th>average_departure_cancelled</th>\n",
       "      <th>average_arrival_diverted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airport_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABE</th>\n",
       "      <td>11.945071</td>\n",
       "      <td>5.558260</td>\n",
       "      <td>0.020873</td>\n",
       "      <td>0.004562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABI</th>\n",
       "      <td>8.027259</td>\n",
       "      <td>5.784016</td>\n",
       "      <td>0.020277</td>\n",
       "      <td>0.000989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABQ</th>\n",
       "      <td>8.635997</td>\n",
       "      <td>5.599697</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABR</th>\n",
       "      <td>7.742198</td>\n",
       "      <td>3.716621</td>\n",
       "      <td>0.010738</td>\n",
       "      <td>0.002685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABY</th>\n",
       "      <td>15.052261</td>\n",
       "      <td>10.642137</td>\n",
       "      <td>0.010806</td>\n",
       "      <td>0.006876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WYS</th>\n",
       "      <td>-2.485356</td>\n",
       "      <td>3.063291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XNA</th>\n",
       "      <td>10.985233</td>\n",
       "      <td>5.628207</td>\n",
       "      <td>0.019728</td>\n",
       "      <td>0.001511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YAK</th>\n",
       "      <td>-8.720506</td>\n",
       "      <td>-3.577247</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>0.009695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YNG</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YUM</th>\n",
       "      <td>-0.455669</td>\n",
       "      <td>-0.316860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>358 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              average_departure_delay  average_arrival_delay  \\\n",
       "airport_code                                                   \n",
       "ABE                         11.945071               5.558260   \n",
       "ABI                          8.027259               5.784016   \n",
       "ABQ                          8.635997               5.599697   \n",
       "ABR                          7.742198               3.716621   \n",
       "ABY                         15.052261              10.642137   \n",
       "...                               ...                    ...   \n",
       "WYS                         -2.485356               3.063291   \n",
       "XNA                         10.985233               5.628207   \n",
       "YAK                         -8.720506              -3.577247   \n",
       "YNG                         63.000000              59.500000   \n",
       "YUM                         -0.455669              -0.316860   \n",
       "\n",
       "              average_departure_cancelled  average_arrival_diverted  \n",
       "airport_code                                                         \n",
       "ABE                              0.020873                  0.004562  \n",
       "ABI                              0.020277                  0.000989  \n",
       "ABQ                              0.009897                  0.001747  \n",
       "ABR                              0.010738                  0.002685  \n",
       "ABY                              0.010806                  0.006876  \n",
       "...                                   ...                       ...  \n",
       "WYS                              0.000000                  0.008368  \n",
       "XNA                              0.019728                  0.001511  \n",
       "YAK                              0.013850                  0.009695  \n",
       "YNG                              0.000000                  0.000000  \n",
       "YUM                              0.000000                  0.000000  \n",
       "\n",
       "[358 rows x 4 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_X_df.isna().sum().sum() + Y.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

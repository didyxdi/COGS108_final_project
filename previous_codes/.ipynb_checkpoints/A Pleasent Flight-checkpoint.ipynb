{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Pleasent Flight\n",
    " Historical Air Crash + Delay Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunely, there were many aviation accidents that caused innocent people to perish. While lessons we learnt in the past tragedy have made flying now more and more safer, the figures of planes falling from the sky still frightens a few people from having a pleasent flight. In this data analysis, we are going persuade thoes who are afraid by to investigate how safe are planes and, if one is still concerned, how to maximize one's safety? To make our report also interesting to general public, we will also investigate how one can minimize delays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan as Nan\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from string import digits\n",
    "import wget # you need to \"pip install wget\"  \n",
    "import glob\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "import xml.etree.ElementTree as ET # to read one dataset in XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Air Crash Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/crash/AviationData.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ca063ef44302>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_storage\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhave_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0metree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datasets/crash/AviationData.xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mxml_root\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Data science\\lib\\xml\\etree\\ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(source, parser)\u001b[0m\n\u001b[0;32m   1195\u001b[0m     \"\"\"\n\u001b[0;32m   1196\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m     \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Data science\\lib\\xml\\etree\\ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, source, parser)\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m             \u001b[0mclose_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/crash/AviationData.xml'"
     ]
    }
   ],
   "source": [
    "use_storage = True\n",
    "have_storage = True\n",
    "\n",
    "try:\n",
    "    pd.read_csv(\"processed/NTSB_crash_df.csv\")\n",
    "except:\n",
    "    have_storage = False\n",
    "\n",
    "if not(use_storage and have_storage):\n",
    "    etree = ET.parse(\"datasets/crash/AviationData.xml\")\n",
    "    xml_root = etree.getroot()\n",
    "\n",
    "    interest_columns = [\"EventId\",\"EventDate\",\"Location\",\"Country\",\"Latitude\",\"Longitude\",\"AirportCode\",\"InjurySeverity\",\"AircraftDamage\",\n",
    "                        \"AircraftCategory\",\"NumberOfEngines\",\"EngineType\",\"Schedule\",\"TotalUninjured\",\"TotalMinorInjuries\",\n",
    "                        \"TotalSeriousInjuries\",\"TotalFatalInjuries\",\"WeatherCondition\",\"BroadPhaseOfFlight\",\"RegistrationNumber\",\"PurposeOfFlight\"]\n",
    "\n",
    "    NTSB_crash_df = pd.DataFrame(columns=interest_columns)\n",
    "\n",
    "    for elem in xml_root: # This loop will run only once\n",
    "        for row in elem: \n",
    "            if not(row.attrib[\"PurposeOfFlight\"]==\"Business\"):\n",
    "                continue\n",
    "            information = list()\n",
    "            for interest in interest_columns:\n",
    "                if not (row.attrib[interest]==\"\"):\n",
    "                    information.append(row.attrib[interest])\n",
    "                else:\n",
    "                    information.append(Nan)\n",
    "            row_information = pd.Series(information,index=interest_columns)\n",
    "            NTSB_crash_df = NTSB_crash_df.append(row_information,ignore_index=True)\n",
    "    NTSB_crash_df.to_csv(\"processed/NTSB_crash_df.csv\")\n",
    "else:\n",
    "    NTSB_crash_df = pd.read_csv(\"processed/NTSB_crash_df.csv\",index_col=0)\n",
    "# this may need to run for a while. It takes 18 seconds on my computer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTSB_crash_df[(NTSB_crash_df[\"AircraftCategory\"]==\"Airplane\") & (NTSB_crash_df[\"EngineType\"]!=\"Reciprocating\") & (NTSB_crash_df[\"Country\"]==\"United States\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Flight Delay Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_storage = True\n",
    "have_storage = True\n",
    "\n",
    "try:\n",
    "    pd.read_csv(\"processed/unpleasant_2009.csv\")\n",
    "except:\n",
    "    have_storage = False\n",
    "\n",
    "if not(use_storage and have_storage):\n",
    "    delay_2009_df = pd.read_csv(\"datasets/delay/2009.csv\")\n",
    "    # DEPARTURE\n",
    "    unpleasant_2009_departure = pd.DataFrame()\n",
    "    unpleasant_2009_departure['total_departure'] = delay_2009_df.loc[:,[\"ORIGIN\"]].groupby('ORIGIN').size()\n",
    "    unpleasant_2009_departure[[\"average_departure_delay\",\"average_departure_taxi\"]] = delay_2009_df.loc[:,[\"ORIGIN\",\"DEP_DELAY\",\"TAXI_OUT\"]].groupby('ORIGIN').mean()\n",
    "    unpleasant_2009_departure['average_departure_cancelled'] = delay_2009_df.loc[:,[\"ORIGIN\",\"CANCELLED\"]].groupby('ORIGIN').mean()\n",
    "    unpleasant_2009_departure['averge_departure_distance'] = delay_2009_df.loc[:,[\"ORIGIN\",\"DISTANCE\"]].groupby('ORIGIN').mean()\n",
    "    #ARRIVAL\n",
    "    unpleasant_2009_arrival = pd.DataFrame()\n",
    "    unpleasant_2009_arrival['total_arrival'] = delay_2009_df.loc[:,[\"DEST\"]].groupby('DEST').size()\n",
    "    unpleasant_2009_arrival[[\"average_arrival_delay\",\"average_arrival_taxi\"]] = delay_2009_df.loc[:,[\"DEST\",\"ARR_DELAY\",\"TAXI_IN\"]].groupby('DEST').mean()\n",
    "    unpleasant_2009_arrival['average_arrival_diverted'] = delay_2009_df.loc[:,[\"DEST\",\"DIVERTED\"]].groupby('DEST').mean()\n",
    "    unpleasant_2009_arrival['averge_arrival_distance'] = delay_2009_df.loc[:,[\"DEST\",\"DISTANCE\"]].groupby('DEST').mean()\n",
    "    \n",
    "    unpleasant_2009_departure['total_departure_lg10'] = unpleasant_2009_departure['total_departure'].apply(np.log10)\n",
    "    unpleasant_2009_arrival['total_arrival_lg10'] = unpleasant_2009_arrival['total_arrival'].apply(np.log10)\n",
    "\n",
    "    unpleasant_2009 = unpleasant_2009_departure.merge(unpleasant_2009_arrival,left_index=True,right_index=True)\n",
    "    unpleasant_2009.to_csv(\"processed/unpleasant_2009.csv\")\n",
    "else:\n",
    "    unpleasant_2009 = pd.read_csv(\"processed/unpleasant_2009.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(unpleasant_2009,figsize=(30,30))\n",
    "plt.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = pd.read_csv(\"datasets/airports/airports-extended.csv\") #https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals#airports-extended.csv\n",
    "df_airport.columns = [\"ID\",\"name\",\"city\",\"country\",\"code\",\"code4\",\"latitude\",\"longitude\",\"altitude_ft\",\"UTC_offset\",\"DST\",\"timezone\",\"type\",\"information_source\"]\n",
    "df_airport = df_airport.loc[:,[\"city\",\"country\",\"code\",\"latitude\",\"longitude\",\"altitude_ft\"]]\n",
    "df_airport = df_airport[df_airport[\"country\"]==\"United States\"]\n",
    "df_airport = df_airport.loc[:,[\"city\",\"code\",\"latitude\",\"longitude\",\"altitude_ft\"]]\n",
    "df_airport = df_airport[df_airport[\"code\"]!=\"\\\\N\"] # remove NAN in index\n",
    "df_airport = df_airport.set_index(\"code\")\n",
    "\n",
    "#clean city name\n",
    "def clean_city_name(input_city):\n",
    "    original = input_city\n",
    "    input_city = str(input_city)\n",
    "    input_city = input_city.strip()\n",
    "    input_city = input_city.lower()\n",
    "    \n",
    "    input_city = input_city.replace(\".\",\"\")\n",
    "    input_city = input_city.replace(\"\\\\\\\\\",\"\")\n",
    "    input_city = input_city.replace(\"-\",\" \")\n",
    "    input_city = input_city.replace(\" - \",\" \")\n",
    "    input_city = input_city.replace(\"saint \",\"st\")\n",
    "    input_city = input_city.replace(\"east \",\"\")\n",
    "    input_city = input_city.replace(\"west \",\"\")\n",
    "    \n",
    "    input_city = input_city.translate({ord(k): None for k in digits})\n",
    "    \n",
    "    if ('/' in input_city):\n",
    "        input_city = input_city[:input_city.find('/')]\n",
    "    if ('(' in input_city):\n",
    "        input_city = input_city[:input_city.find('(')]\n",
    "    if (',' in input_city):\n",
    "        input_city = input_city[:input_city.find(',')]\n",
    "    input_city = input_city.strip()   \n",
    "    if (' ' in input_city):\n",
    "        temp=input_city.find(' ')\n",
    "        if (temp > 2):\n",
    "            input_city = input_city[:input_city.find(' ')]\n",
    "        else:\n",
    "            if (input_city.find(' ',temp+1) != -1):\n",
    "                input_city = input_city[temp+1:input_city.find(' ',temp+1)]\n",
    "            else:\n",
    "                input_city = input_city[temp+1:]\n",
    "    input_city = input_city.strip()\n",
    "    try:\n",
    "        assert len(input_city) > 2\n",
    "        assert input_city.replace(\" \",\"\").replace(\"'\",\"\").isalpha()\n",
    "    except:\n",
    "        print(\"This city name is prehaps incorrect: \",original,input_city,len(original))\n",
    "    return input_city\n",
    "\n",
    "df_airport[\"city\"] = df_airport[\"city\"].apply(clean_city_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean city climate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Match airport with county information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df = pd.read_csv(\"datasets/city/uscities.csv\")\n",
    "\n",
    "us_cities_df = us_cities_df[[\"city\",\"state_id\",\"county_fips\",\"county_name\",\"population\",\"density\",\"lat\",\"lng\"]]\n",
    "us_cities_df[\"fips\"] = us_cities_df[\"county_fips\"]\n",
    "\n",
    "airport_prop_df = unpleasant_2009.merge(df_airport,how='inner',left_index=True,right_index=True)\n",
    "\n",
    "def get_county_code(input_county):\n",
    "    return int(input_county) % 1000\n",
    "\n",
    "us_cities_df[\"county_fips\"] = us_cities_df[\"county_fips\"].apply(get_county_code)\n",
    "\n",
    "us_cities_df[\"city\"] = us_cities_df[\"city\"].apply(clean_city_name)\n",
    "\n",
    "\n",
    "city_climate_df = pd.DataFrame(columns=[\"code\",\"population\",\"density\",\"avg_temp_sp\",\"avg_temp_su\",\"avg_temp_fa\",\"avg_temp_wi\",\"avg_precipitation_sp\",\"avg_precipitation_su\",\"avg_precipitation_fa\",\"avg_precipitation_wi\"])\n",
    "city_search_df = pd.DataFrame(columns=[\"code\",\"state_id\",\"county_id\",\"city_id\",\"fips\"])\n",
    "for ind,row in airport_prop_df.iterrows():\n",
    "    city = row[\"city\"]\n",
    "    target_lat = row[\"latitude\"]\n",
    "    target_lng = row[\"longitude\"]\n",
    "    try:      \n",
    "        target_cities = us_cities_df[us_cities_df[\"city\"]==city]\n",
    "        \n",
    "        if not (target_cities.shape[0] == 1):\n",
    "            def calc_dis(input_):\n",
    "                err = abs(target_lat - input_[\"lat\"]) + abs(target_lng - input_[\"lng\"])\n",
    "                return err\n",
    "            target_cities.loc[:,\"error\"] = (target_cities.apply(calc_dis,axis=1))\n",
    "            target_city = target_cities.sort_values(by=\"error\").iloc[0]\n",
    "            \n",
    "            assert target_city[\"error\"] < 1.5\n",
    "            \n",
    "            target_city = target_city.drop([\"error\"])\n",
    "        elif (target_cities.shape[0] >= 1):\n",
    "            target_city = target_cities.iloc[0]\n",
    "        \n",
    "        county = str(target_city[\"county_fips\"])\n",
    "        if (len(county)==1):\n",
    "            county = \"00\" + county\n",
    "        elif (len(county)==2):\n",
    "            county = \"0\" + county\n",
    "\n",
    "        city_search_df = city_search_df.append({\"code\":ind,\"state_id\":target_city[\"state_id\"],\"county_id\":county,\"city_id\":target_city.name,\"fips\":target_city[\"fips\"]},ignore_index=True)        \n",
    "    except:\n",
    "        try:\n",
    "            def calc_dis(input_):\n",
    "                err = abs(target_lat - input_[\"lat\"]) + abs(target_lng - input_[\"lng\"])\n",
    "                return err\n",
    "            us_cities_df_copy = us_cities_df\n",
    "            us_cities_df_copy.loc[:,\"error\"] = (us_cities_df.apply(calc_dis,axis=1))\n",
    "            target_city = us_cities_df_copy.sort_values(by=\"error\").iloc[0]\n",
    "            assert target_city[\"error\"] < 1.5\n",
    "            county = str(target_city[\"county_fips\"])\n",
    "            if (len(county)==1):\n",
    "                county = \"00\" + county\n",
    "            elif (len(county)==2):\n",
    "                county = \"0\" + county\n",
    "            city_search_df = city_search_df.append({\"code\":ind,\"state_id\":target_city[\"state_id\"],\"county_id\":county,\"city_id\":target_city.name,\"fips\":target_city[\"fips\"]},ignore_index=True)\n",
    "        except:\n",
    "            print(\"No data for \",city)\n",
    "            city_search_df = city_search_df.append({\"code\":ind,\"state_id\":np.nan,\"county_id\":np.nan,\"city_id\":np.nan,\"fips\":np.nan},ignore_index=True)\n",
    "\n",
    "#special case for DC\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    if (row[\"code\"]==\"DCA\"):\n",
    "        city_search_df.iloc[ind][\"state_id\"]=\"MD\"\n",
    "        city_search_df.iloc[ind][\"county_id\"]=\"511\"\n",
    "\n",
    "        \n",
    "        \n",
    "city_search_df = city_search_df[(city_search_df[\"state_id\"]!=\"HI\") & (city_search_df[\"state_id\"]!=\"AK\")]\n",
    "        \n",
    "city_search_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape these cities (counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_climate_data(state,county,year):\n",
    "    save_path = \"datasets/weather/\"\n",
    "    fname = state + county + \"_\" + str(year) + \".csv\"\n",
    "    if (len(glob.glob(save_path + fname))==0):\n",
    "        URL = \"https://www.ncdc.noaa.gov/cag/county/time-series/{}-{}-{}-all-1-2000-2020.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state,county,\"tavg\")\n",
    "        r = requests.get(URL)\n",
    "        file = wget.download(URL,out=save_path + \"tavg/tavg_\" + fname)\n",
    "        URL = \"https://www.ncdc.noaa.gov/cag/county/time-series/{}-{}-{}-all-1-2000-2020.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state,county,\"pcp\")\n",
    "        r = requests.get(URL)\n",
    "        file = wget.download(URL,out=save_path + \"pcp/pcp_\" + fname)\n",
    "\n",
    "        df_tavg = pd.read_csv(save_path + \"tavg/tavg_\" + fname).iloc[4:]\n",
    "        df_tavg.columns=[\"date\",\"tavg\",\"comp\"]\n",
    "        tavg = df_tavg.set_index(\"date\")[\"tavg\"]\n",
    "\n",
    "        df_pcp = pd.read_csv(save_path + \"pcp/pcp_\" + fname).iloc[4:]\n",
    "        df_pcp.columns=[\"date\",\"pcp\",\"comp\"]\n",
    "        pcp = df_pcp.set_index(\"date\")[\"pcp\"]\n",
    "\n",
    "        pd.concat([tavg, pcp], axis=1).to_csv(save_path + fname)\n",
    "        time.sleep(1) # not requesting too frequently\n",
    "\n",
    "counter = 0\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    try:\n",
    "        download_climate_data(row[\"state_id\"],row[\"county_id\"],2018)\n",
    "        download_climate_data(row[\"state_id\"],row[\"county_id\"],2019)\n",
    "    except:\n",
    "        print(row)\n",
    "    counter+=1\n",
    "    print(\"progress: {:.2f}%   Just done: {}\".format(100 * counter / city_search_df.shape[0],row[\"code\"]),end=\"\\r\")\n",
    "\n",
    "\n",
    "temp_pcp_df = pd.DataFrame(columns=[\"code\",\"temp_avg\",\"pcp_avg\"])\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    state = row[\"state_id\"]\n",
    "    county = row[\"county_id\"]\n",
    "    save_path = \"datasets/weather/\"\n",
    "    years = [2018,2019]\n",
    "    \n",
    "    try:\n",
    "        tavg = 0\n",
    "        pcp = 0\n",
    "        for year in years:\n",
    "            fname = state + county + \"_\" + str(year) + \".csv\"\n",
    "            temp_pcp = pd.read_csv(save_path + fname)\n",
    "            tavg += temp_pcp.mean()[\"tavg\"]\n",
    "            pcp += temp_pcp.mean()[\"pcp\"]\n",
    "        tavg /= len(years)\n",
    "        pcp /= len(years)\n",
    "        \n",
    "        temp_pcp_df = temp_pcp_df.append({\"code\":row[\"code\"],\"temp_avg\":tavg,\"pcp_avg\":pcp},ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        temp_pcp_df = temp_pcp_df.append({\"code\":row[\"code\"],\"temp_avg\":np.nan,\"pcp_avg\":np.nan},ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df = airport_prop_df.merge(city_search_df.set_index(\"code\").loc[:,[\"city_id\",\"fips\"]],left_index=True,right_index=True)\n",
    "airport_prop_df = airport_prop_df.merge(temp_pcp_df.set_index(\"code\"),left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "airport_prop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale = [\"#f7fbff\", \"#ebf3fb\", \"#deebf7\", \"#d2e3f3\", \"#c6dbef\", \"#b3d2e9\", \"#9ecae1\",\n",
    "    \"#85bcdb\", \"#6baed6\", \"#57a0ce\", \"#4292c6\", \"#3082be\", \"#2171b5\", \"#1361a9\",\n",
    "    \"#08519c\", \"#0b4083\", \"#08306b\"\n",
    "]\n",
    "values = airport_prop_df['pcp_avg'].tolist()\n",
    "endpts = list(np.linspace(np.min(values), np.max(values), len(colorscale) - 1))\n",
    "fips = airport_prop_df['fips'].tolist()\n",
    "\n",
    "\n",
    "fig = ff.create_choropleth(\n",
    "    fips=fips, values=values, scope=['usa'],\n",
    "    binning_endpoints=endpts, colorscale=colorscale,\n",
    "    show_state_data=False,\n",
    "    show_hover=True,\n",
    "    asp = 2.9,\n",
    "    title_text = 'PCP_avg',\n",
    "    legend_title = 'in'\n",
    ")\n",
    "fig.layout.template = None\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bubble_map(df,column,limits,color,title,scale=1):\n",
    "\n",
    "    if not (column in df.columns):\n",
    "        print(\"column \\\"\"+column+\"\\\"\",\"not found in dataframe\")\n",
    "        return\n",
    "    \n",
    "    if not ('longitude' in df.columns and 'latitude' in df.columns):\n",
    "        print(\"longitude or latitude not present in df\")\n",
    "        return\n",
    "    if not (limits[0][0] <= df[column].min()):\n",
    "        print(\"lower limit is higher then some of the rows\")\n",
    "        print(\"df[\" + column + \"].min()=\",df[column].min())\n",
    "    \n",
    "    if not (limits[len(limits)-1][1] >= df[column].max()):\n",
    "        print(\"upper limit is lower then some of the rows\")\n",
    "        print(\"df[\" + column + \"].max()=\",df[column].max())\n",
    "    \n",
    "    #preprocess color\n",
    "    colorseries={\n",
    "        \"blue\":[\"#f7fbff\", \"#ebf3fb\", \"#d2e3f3\", \"#c6dbef\", \"#b3d2e9\", \"#9ecae1\",\n",
    "                \"#85bcdb\", \"#6baed6\", \"#57a0ce\", \"#3082be\", \"#2171b5\", \"#1361a9\",\n",
    "                \"#08519c\", \"#0b4083\", \"#08306b\"],\n",
    "        \"gray\":[\"#ffffff\",\"#eeeeee\",\"#dddddd\",\"#cccccc\",\"#bbbbbb\",\"#aaaaaa\",\"#999999\",\"#888888\"],\n",
    "        \"thermal\":[\"#00000a\",\"#000014\",\"#00001e\",\"#000025\",\"#00002a\",\"#00002e\",\"#000032\",\"#000036\",\"#00003a\",\"#00003e\",\"#000042\",\"#000046\",\"#00004a\",\"#00004f\",\"#000052\",\"#010055\",\"#010057\",\"#020059\",\"#02005c\",\"#03005e\",\"#040061\",\"#040063\",\"#050065\",\"#060067\",\"#070069\",\"#08006b\",\"#09006e\",\"#0a0070\",\"#0b0073\",\"#0c0074\",\"#0d0075\",\"#0d0076\",\"#0e0077\",\"#100078\",\"#120079\",\"#13007b\",\"#15007c\",\"#17007d\",\"#19007e\",\"#1b0080\",\"#1c0081\",\"#1e0083\",\"#200084\",\"#220085\",\"#240086\",\"#260087\",\"#280089\",\"#2a0089\",\"#2c008a\",\"#2e008b\",\"#30008c\",\"#32008d\",\"#34008e\",\"#36008e\",\"#38008f\",\"#390090\",\"#3b0091\",\"#3c0092\",\"#3e0093\",\"#3f0093\",\"#410094\",\"#420095\",\"#440095\",\"#450096\",\"#470096\",\"#490096\",\"#4a0096\",\"#4c0097\",\"#4e0097\",\"#4f0097\",\"#510097\",\"#520098\",\"#540098\",\"#560098\",\"#580099\",\"#5a0099\",\"#5c0099\",\"#5d009a\",\"#5f009a\",\"#61009b\",\"#63009b\",\"#64009b\",\"#66009b\",\"#68009b\",\"#6a009b\",\"#6c009c\",\"#6d009c\",\"#6f009c\",\"#70009c\",\"#71009d\",\"#73009d\",\"#75009d\",\"#77009d\",\"#78009d\",\"#7a009d\",\"#7c009d\",\"#7e009d\",\"#7f009d\",\"#81009d\",\"#83009d\",\"#84009d\",\"#86009d\",\"#87009d\",\"#89009d\",\"#8a009d\",\"#8b009d\",\"#8d009d\",\"#8f009c\",\"#91009c\",\"#93009c\",\"#95009c\",\"#96009b\",\"#98009b\",\"#99009b\",\"#9b009b\",\"#9c009b\",\"#9d009b\",\"#9f009b\",\"#a0009b\",\"#a2009b\",\"#a3009b\",\"#a4009b\",\"#a6009a\",\"#a7009a\",\"#a8009a\",\"#a90099\",\"#aa0099\",\"#ab0099\",\"#ad0099\",\"#ae0198\",\"#af0198\",\"#b00198\",\"#b00198\",\"#b10197\",\"#b20197\",\"#b30196\",\"#b40296\",\"#b50295\",\"#b60295\",\"#b70395\",\"#b80395\",\"#b90495\",\"#ba0495\",\"#ba0494\",\"#bb0593\",\"#bc0593\",\"#bd0593\",\"#be0692\",\"#bf0692\",\"#bf0692\",\"#c00791\",\"#c00791\",\"#c10890\",\"#c10990\",\"#c20a8f\",\"#c30a8e\",\"#c30b8e\",\"#c40c8d\",\"#c50c8c\",\"#c60d8b\",\"#c60e8a\",\"#c70f89\",\"#c81088\",\"#c91187\",\"#ca1286\",\"#ca1385\",\"#cb1385\",\"#cb1484\",\"#cc1582\",\"#cd1681\",\"#ce1780\",\"#ce187e\",\"#cf187c\",\"#cf197b\",\"#d01a79\",\"#d11b78\",\"#d11c76\",\"#d21c75\",\"#d21d74\",\"#d31e72\",\"#d32071\",\"#d4216f\",\"#d4226e\",\"#d5236b\",\"#d52469\",\"#d62567\",\"#d72665\",\"#d82764\",\"#d82862\",\"#d92a60\",\"#da2b5e\",\"#da2c5c\",\"#db2e5a\",\"#db2f57\",\"#dc2f54\",\"#dd3051\",\"#dd314e\",\"#de324a\",\"#de3347\",\"#df3444\",\"#df3541\",\"#df363d\",\"#e0373a\",\"#e03837\",\"#e03933\",\"#e13a30\",\"#e23b2d\",\"#e23c2a\",\"#e33d26\",\"#e33e23\",\"#e43f20\",\"#e4411d\",\"#e4421c\",\"#e5431b\",\"#e54419\",\"#e54518\",\"#e64616\",\"#e74715\",\"#e74814\",\"#e74913\",\"#e84a12\",\"#e84c10\",\"#e84c0f\",\"#e94d0e\",\"#e94d0d\",\"#ea4e0c\",\"#ea4f0c\",\"#eb500b\",\"#eb510a\",\"#eb520a\",\"#eb5309\",\"#ec5409\",\"#ec5608\",\"#ec5708\",\"#ec5808\",\"#ed5907\",\"#ed5a07\",\"#ed5b06\",\"#ee5c06\",\"#ee5c05\",\"#ee5d05\",\"#ee5e05\",\"#ef5f04\",\"#ef6004\",\"#ef6104\",\"#ef6204\",\"#f06303\",\"#f06403\",\"#f06503\",\"#f16603\",\"#f16603\",\"#f16703\",\"#f16803\",\"#f16902\",\"#f16a02\",\"#f16b02\",\"#f16b02\",\"#f26c01\",\"#f26d01\",\"#f26e01\",\"#f36f01\",\"#f37001\",\"#f37101\",\"#f37201\",\"#f47300\",\"#f47400\",\"#f47500\",\"#f47600\",\"#f47700\",\"#f47800\",\"#f47a00\",\"#f57b00\",\"#f57c00\",\"#f57e00\",\"#f57f00\",\"#f68000\",\"#f68100\",\"#f68200\",\"#f78300\",\"#f78400\",\"#f78500\",\"#f78600\",\"#f88700\",\"#f88800\",\"#f88800\",\"#f88900\",\"#f88a00\",\"#f88b00\",\"#f88c00\",\"#f98d00\",\"#f98d00\",\"#f98e00\",\"#f98f00\",\"#f99000\",\"#f99100\",\"#f99200\",\"#f99300\",\"#fa9400\",\"#fa9500\",\"#fa9600\",\"#fb9800\",\"#fb9900\",\"#fb9a00\",\"#fb9c00\",\"#fc9d00\",\"#fc9f00\",\"#fca000\",\"#fca100\",\"#fda200\",\"#fda300\",\"#fda400\",\"#fda600\",\"#fda700\",\"#fda800\",\"#fdaa00\",\"#fdab00\",\"#fdac00\",\"#fdad00\",\"#fdae00\",\"#feaf00\",\"#feb000\",\"#feb100\",\"#feb200\",\"#feb300\",\"#feb400\",\"#feb500\",\"#feb600\",\"#feb800\",\"#feb900\",\"#feb900\",\"#feba00\",\"#febb00\",\"#febc00\",\"#febd00\",\"#febe00\",\"#fec000\",\"#fec100\",\"#fec200\",\"#fec300\",\"#fec400\",\"#fec500\",\"#fec600\",\"#fec700\",\"#fec800\",\"#fec901\",\"#feca01\",\"#feca01\",\"#fecb01\",\"#fecc02\",\"#fecd02\",\"#fece03\",\"#fecf04\",\"#fecf04\",\"#fed005\",\"#fed106\",\"#fed308\",\"#fed409\",\"#fed50a\",\"#fed60a\",\"#fed70b\",\"#fed80c\",\"#fed90d\",\"#ffda0e\",\"#ffda0e\",\"#ffdb10\",\"#ffdc12\",\"#ffdc14\",\"#ffdd16\",\"#ffde19\",\"#ffde1b\",\"#ffdf1e\",\"#ffe020\",\"#ffe122\",\"#ffe224\",\"#ffe226\",\"#ffe328\",\"#ffe42b\",\"#ffe42e\",\"#ffe531\",\"#ffe635\",\"#ffe638\",\"#ffe73c\",\"#ffe83f\",\"#ffe943\",\"#ffea46\",\"#ffeb49\",\"#ffeb4d\",\"#ffec50\",\"#ffed54\",\"#ffee57\",\"#ffee5b\",\"#ffee5f\",\"#ffef63\",\"#ffef67\",\"#fff06a\",\"#fff06e\",\"#fff172\",\"#fff177\",\"#fff17b\",\"#fff280\",\"#fff285\",\"#fff28a\",\"#fff38e\",\"#fff492\",\"#fff496\",\"#fff49a\",\"#fff59e\",\"#fff5a2\",\"#fff5a6\",\"#fff6aa\",\"#fff6af\",\"#fff7b3\",\"#fff7b6\",\"#fff8ba\",\"#fff8bd\",\"#fff8c1\",\"#fff8c4\",\"#fff9c7\",\"#fff9ca\",\"#fff9cd\",\"#fffad1\",\"#fffad4\",\"#fffbd8\",\"#fffcdb\",\"#fffcdf\",\"#fffde2\",\"#fffde5\",\"#fffde8\",\"#fffeeb\",\"#fffeee\",\"#fffef1\",\"#fffef4\",\"#fffff6\"],\n",
    "        \"black\":[\"#bbbbbb\",\"#aaaaaa\",\"#999999\",\"#888888\",\"#777777\",\"#666666\",\"#555555\",\"#444444\",\"#333333\",\"#222222\",\"#111111\",\"#000000\"]\n",
    "    }\n",
    "    colors = list()\n",
    "    inds = list(np.linspace(0,len(colorseries[color])-1,len(limits)).astype(\"int\"))\n",
    "    for i in inds:\n",
    "        colors.append(colorseries[color][i])\n",
    "        \n",
    "    scale *= 250 / (df[column].max() - df[column].min())\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for i in range(len(limits)):\n",
    "        lim = limits[i]\n",
    "        df_sub = df[(lim[0]<=df[column]) & (df[column]<lim[1])]\n",
    "        fig.add_trace(go.Scattergeo(\n",
    "            locationmode = 'USA-states',\n",
    "            lon = df_sub['longitude'],\n",
    "            lat = df_sub['latitude'],\n",
    "            text = df_sub.index,\n",
    "            marker = dict(\n",
    "                size = (df_sub[column] - df[column].min())*scale, # ensure size is positive\n",
    "                color = colors[i],\n",
    "                line_color='rgb(40,40,40)',\n",
    "                line_width=0,\n",
    "                sizemode = 'area'\n",
    "            ),\n",
    "            name = '{0} - {1}'.format(lim[0],lim[1])))\n",
    "\n",
    "    fig.update_layout(\n",
    "            title_text = title,\n",
    "            showlegend = True,\n",
    "            geo = dict(\n",
    "                scope = 'usa',\n",
    "                landcolor = 'rgb(217, 217, 217)',\n",
    "            )\n",
    "        )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(airport_prop_df,\n",
    "                          column=\"average_departure_delay\",\n",
    "                          limits=[(-10,0),(0,5),(5,10),(10,20),(20,50)],\n",
    "                          color=\"blue\",\n",
    "                          title='2009 United States airport departure delay')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(airport_prop_df,\n",
    "                          column=\"pcp_avg\",\n",
    "                          limits=[(0,1.4),(1.4,2.8),(2.8,4.2),(4.2,5.6),(5.6,7.0)],\n",
    "                          color=\"blue\",\n",
    "                          title='United States precipitation average')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(airport_prop_df,\n",
    "                          column=\"temp_avg\",\n",
    "                          limits=[(32,36),(36,40),(40,44),(44,48),(48,52),(52,56),(56,60),(60,64),(64,68),(68,72),(72,76),(76,80)],\n",
    "                          color=\"thermal\",\n",
    "                          title='United States temperature average')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(airport_prop_df,\n",
    "                          column=\"total_departure\",\n",
    "                          limits=[(0,40000),(40000,140000),(140000,240000),(240000,340000),(340000,420000)],\n",
    "                          color=\"blue\",\n",
    "                          title='United States total departure')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(airport_prop_df,\n",
    "                          column=\"average_departure_cancelled\",\n",
    "                          limits=[(0,0.01),(0.01,0.02),(0.02,0.03),(0.03,0.04),(0.04,0.05),(0.05,0.06),(0.06,0.07),(0.07,0.08),(0.08,0.09),(0.09,0.11)],\n",
    "                          color=\"blue\",\n",
    "                          title='United States average departure calcelled')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(airport_prop_df,\n",
    "                          column=\"average_departure_taxi\",\n",
    "                          limits=[(0,3),(3,6),(6,9),(9,15),(15,21),(21,40)],\n",
    "                          color=\"blue\",\n",
    "                          title='United States average departure taxi time')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(airport_prop_df,\n",
    "                          column=\"altitude_ft\",\n",
    "                          limits=[(-54,1500),(1500,3000),(3000,4500),(6000,7500),(7500,9000),(9000,10500)],\n",
    "                          color=\"blue\",\n",
    "                          title='United States altitude_ft')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File datasets/delay/2009.csv does not exist: 'datasets/delay/2009.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d30b1df14b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdelay_2009_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datasets/delay/2009.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Data science\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Data science\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Data science\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Data science\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Data science\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File datasets/delay/2009.csv does not exist: 'datasets/delay/2009.csv'"
     ]
    }
   ],
   "source": [
    "delay_2009_df = pd.read_csv(\"datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

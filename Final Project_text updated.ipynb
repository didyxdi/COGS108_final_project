{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Ivan Jin (A14880280)\n",
    " - Hongsheng Xie (A14794772)\n",
    " - Tong Wang (A13713688)\n",
    " - Yinxuan Du (A15873678)\n",
    " - Yuchen Zhang (A16151373)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Question Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background & Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BG + Prior work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Airline Delay and Cancellation Data, 2009 - 2018\n",
    "- Content: US domestic flight delay data\n",
    "- Columns of interest: \n",
    "- \\# of observations: 6429338 rows for year 2009\n",
    "- Source: https://www.kaggle.com/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018\n",
    "- Function: main outcome variable\n",
    "\n",
    "### 2. FILL ME ACCORDING TO OUR DATASET!\n",
    "- Content:\n",
    "- Columns of interest: \n",
    "- \\# of observations: \n",
    "- Source: \n",
    "- Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0378ef703a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan as Nan\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import requests\n",
    "from string import digits\n",
    "import wget \n",
    "import glob\n",
    "import time\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "import patsy\n",
    "import psutil\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "import xml.etree.ElementTree as ET # to read one dataset in XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer our question, we need to clean and aggregate our main outcome variable (delay, crash) and our predictors by airports. The goal of data cleaning is to get two dataframes that have airport code as index:\n",
    " - 1.Y \n",
    "     - columns\n",
    "         - average_departure_delay - average delay of all flights originated from this airport \n",
    "         - average_arrival_delay - average delay of all flights arriving at this airport\n",
    "         - average_cancelled - average cancel rate of all flights originated from this airport\n",
    "         - average_diverted - average diverted rate of all flights arriving at this airport\n",
    " - 2.X \n",
    "     - columns\n",
    "         - code4 - 4 digit airport code\n",
    "         - city_name - name of the city that located near the airport\n",
    "         - city_id - index of this city in uscities_df\n",
    "         - latitude - latitude of the airport\n",
    "         - longitude - longitude of the airport\n",
    "         - altitude_ft - altitude of the airport\n",
    "         - temp_avg - 2018-2019 average tempreature of the county contining the airport\n",
    "         - pcp_avg - 2018-2019 average precipitation of the county contining the airport\n",
    "         - strike_avg - year-average number bird strike (may not caused damage)\n",
    "         - damage_avg - year-average number of bird strike that caused damage\n",
    "         - city_population - population the city that located near the airport\n",
    "         - enplanements - enplanements of the city that located near the airport\n",
    "         - length_ft_sum - sum of length of all runways\n",
    "         - width_ft_avg - average width of all runways\n",
    "         - num_runways - number of runways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning - Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Delay dataset - departure/ arrival delay + cancel / divert rate \n",
    "From `A Pleasant Flight.ipynb` (now in the folder `previous_codes`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_2018_df = pd.read_csv(\"datasets/original/delay/2018.csv\")\n",
    "delay_2018_df = delay_2018_df[[\"ORIGIN\",\"DEST\",\"DEP_DELAY\",\"TAXI_OUT\",\"CANCELLED\",\"DISTANCE\",\"ARR_DELAY\",\"TAXI_IN\",\"DIVERTED\"]] # columns of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,we check the distribution of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_2018_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values for delay columns. However, we discovered that 95% of the missing value have \"CANCALLED\" == 1. It is logical to have missing delay value when the flight is never done. For these flight have already contributed to the cancel rate feature, thus should not have a effect on the delay. Since we are using the .mean() function, which will ignore all nan values, we left these rows unchanged. Same for diverted, when there shouldn't be a arrival delay because the flight is not arriving at the scheduled airport. We kept them because we can have a more accurate total departure/arrival number. In this way we have only 4000 rows that we can't explain the reason behind missing value. Since we have 7213446 rows in total, 4000 is a acceptable amount of missing. We also kept it for accurate total departure/arrival number.\n",
    "We then calculated departure_delay, arrival_delay and other columns of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPARTURE\n",
    "unpleasant_2018_departure = pd.DataFrame()\n",
    "unpleasant_2018_departure['total_departure'] = delay_2018_df.loc[:,[\"ORIGIN\"]].groupby('ORIGIN').size()\n",
    "unpleasant_2018_departure[[\"average_departure_delay\",\"average_departure_taxi\"]] = delay_2018_df.loc[:,[\"ORIGIN\",\"DEP_DELAY\",\"TAXI_OUT\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2018_departure['average_departure_cancelled'] = delay_2018_df.loc[:,[\"ORIGIN\",\"CANCELLED\"]].groupby('ORIGIN').mean()\n",
    "unpleasant_2018_departure['averge_departure_distance'] = delay_2018_df.loc[:,[\"ORIGIN\",\"DISTANCE\"]].groupby('ORIGIN').mean()\n",
    "    \n",
    "#ARRIVAL\n",
    "unpleasant_2018_arrival = pd.DataFrame()\n",
    "unpleasant_2018_arrival['total_arrival'] = delay_2018_df.loc[:,[\"DEST\"]].groupby('DEST').size()\n",
    "unpleasant_2018_arrival[[\"average_arrival_delay\",\"average_arrival_taxi\"]] = delay_2018_df.loc[:,[\"DEST\",\"ARR_DELAY\",\"TAXI_IN\"]].groupby('DEST').mean()\n",
    "unpleasant_2018_arrival['average_arrival_diverted'] = delay_2018_df.loc[:,[\"DEST\",\"DIVERTED\"]].groupby('DEST').mean()\n",
    "unpleasant_2018_arrival['averge_arrival_distance'] = delay_2018_df.loc[:,[\"DEST\",\"DISTANCE\"]].groupby('DEST').mean()\n",
    "\n",
    "#Add log to total_departure and total_arrival \n",
    "unpleasant_2018_departure['total_departure_lg10'] = unpleasant_2018_departure['total_departure'].apply(np.log10)\n",
    "unpleasant_2018_arrival['total_arrival_lg10'] = unpleasant_2018_arrival['total_arrival'].apply(np.log10)\n",
    "\n",
    "#Merge departure and arrival\n",
    "delay_2018_df = unpleasant_2018_departure.merge(unpleasant_2018_arrival,left_index=True,right_index=True)\n",
    "delay_2018_df.index.names = [\"airport_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope:\n",
    "We are interested in airports in **U.S. mainland** and with at least **10** departures. We dropped those have <10 departures and state Hawaii and Alaska later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_2018_df = delay_2018_df[delay_2018_df[\"total_departure\"]>=10]\n",
    "\n",
    "# Main outcome variable\n",
    "Y = delay_2018_df.loc[:,[\"average_departure_delay\",\"average_arrival_delay\",\"average_departure_cancelled\",\"average_arrival_diverted\"]]\n",
    "\n",
    "#\n",
    "unpleasant_airport_code_df = delay_2018_df[[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning - X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Delay dataset - total arrival / departure + average departure / arrival distance + departure / arrival taxi time\n",
    "From `A Pleasant Flight.ipynb` (now in the folder `previous_codes`), Cleaned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract X features\n",
    "X = pd.DataFrame()\n",
    "X[[\"total_departure\",\"total_arrival\",\"total_departure_lg10\",\"total_arrival_lg10\",\"averge_departure_distance\",\"averge_arrival_distance\",\"average_departure_taxi\",\"average_arrival_taxi\"]] \\\n",
    "= delay_2018_df.loc[:,[\"total_departure\",\"total_arrival\",\"total_departure_lg10\",\"total_arrival_lg10\",\"averge_departure_distance\",\"averge_arrival_distance\",\"average_departure_taxi\",\"average_arrival_taxi\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 U.S. Airport dataset - city name, latitude, longitude, altitude\n",
    "From `A Pleasant Flight.ipynb` (now in the folder `previous_codes`).\n",
    "\n",
    "Clean `datasets/original/airport/airport-extended.csv`. Generate `airport_loc_df` with column names city_name, latitude, longitude, altitude_ft. Export `airport_loc.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df = pd.read_csv(\"datasets/original/airport/airports-extended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_city_name(input_city):\n",
    "    original = input_city\n",
    "    input_city = str(input_city)\n",
    "    input_city = input_city.strip()\n",
    "    input_city = input_city.lower()\n",
    "    \n",
    "    input_city = input_city.replace(\".\",\"\")\n",
    "    input_city = input_city.replace(\"\\\\\\\\\",\"\")\n",
    "    input_city = input_city.replace(\"-\",\" \")\n",
    "    input_city = input_city.replace(\" - \",\" \")\n",
    "    input_city = input_city.replace(\"saint \",\"st\")\n",
    "    input_city = input_city.replace(\"east \",\"\")\n",
    "    input_city = input_city.replace(\"west \",\"\")\n",
    "    \n",
    "    input_city = input_city.translate({ord(k): None for k in digits})\n",
    "    \n",
    "    if ('/' in input_city):\n",
    "        input_city = input_city[:input_city.find('/')] # in case city have muitiple names like \"cityname1/cityname2\"\n",
    "    if ('(' in input_city):\n",
    "        input_city = input_city[:input_city.find('(')] # Same as above\n",
    "    if (',' in input_city):\n",
    "        input_city = input_city[:input_city.find(',')] \n",
    "    input_city = input_city.strip()   \n",
    "    if (' ' in input_city):\n",
    "        temp=input_city.find(' ')\n",
    "        if (temp > 2):\n",
    "            input_city = input_city[:input_city.find(' ')]\n",
    "        else:\n",
    "            if (input_city.find(' ',temp+1) != -1):\n",
    "                input_city = input_city[temp+1:input_city.find(' ',temp+1)]\n",
    "            else:\n",
    "                input_city = input_city[temp+1:]\n",
    "    input_city = input_city.strip()\n",
    "    try:\n",
    "        assert len(input_city) > 2\n",
    "        assert input_city.replace(\" \",\"\").replace(\"'\",\"\").isalpha()\n",
    "    except:\n",
    "        #print(\"This city name is prehaps incorrect: \",original,input_city,len(original))\n",
    "        1+1\n",
    "    return input_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_loc_df.columns = [\"ID\",\"name\",\"city_name\",\"country\",\"airport_code\",\"code4\",\"latitude\",\"longitude\",\"altitude_ft\",\"UTC_offset\",\"DST\",\"timezone\",\"type\",\"information_source\"]\n",
    "airport_loc_df = airport_loc_df.loc[:,[\"city_name\",\"country\",\"airport_code\",\"code4\", \"latitude\",\"longitude\",\"altitude_ft\"]] # add code4 since we need to use it to get runways data --YD 03/05\n",
    "airport_loc_df = airport_loc_df[airport_loc_df[\"country\"]==\"United States\"] # reduce dataframe size\n",
    "airport_loc_df = airport_loc_df.loc[:,[\"city_name\",\"airport_code\",\"code4\", \"latitude\",\"longitude\",\"altitude_ft\"]]# We don't need the country column again\n",
    "airport_loc_df = airport_loc_df[airport_loc_df[\"airport_code\"]!=\"\\\\N\"] # remove NAN in index\n",
    "airport_loc_df = airport_loc_df.set_index(\"airport_code\")\n",
    "airport_loc_df[\"city_name\"] = airport_loc_df[\"city_name\"].apply(clean_city_name)# clean city name\n",
    "\n",
    "# store cleaned data\n",
    "airport_loc_df.to_csv(\"datasets/cleaned/airport_loc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 U.S. Cities dataset - city name, FIPS, population\n",
    "Clean `datasets/original/city/uscities.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df = pd.read_csv(\"datasets/original/city/uscities.csv\")\n",
    "\n",
    "# Kept latitude and longitude to differentiate cities with same name\n",
    "us_cities_df = us_cities_df[[\"city\",\"state_id\",\"county_fips\",\"county_name\",\"population\",\"lat\",\"lng\"]] \n",
    "us_cities_df = us_cities_df.rename(columns = {\"city\":\"city_name\"})                                              \n",
    "us_cities_df[\"fips\"] = us_cities_df[\"county_fips\"]\n",
    "\n",
    "def get_county_code(input_county):\n",
    "    return int(input_county) % 1000\n",
    "\n",
    "us_cities_df[\"county_fips\"] = us_cities_df[\"county_fips\"].apply(get_county_code)\n",
    "us_cities_df[\"city_name\"] = us_cities_df[\"city_name\"].apply(clean_city_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Merge step 2.2 - 2.4\n",
    "Merge `unpleasant_airport_code_df`(empty dataframe with only `airport_code` as index) and `airport_loc_df` by `airport_code` to select the airport we need. Then merge it with `us_cities_df` by `city_name`. Rank duplicaited city name by abs(delta_latitude) + abs(delta_longitude), assert the number is < 1.5 deg and select the city with smallest error.\n",
    "\n",
    "Generate `city_search_df` for scraping climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df = unpleasant_airport_code_df.merge(airport_loc_df,how='inner',left_index=True,right_index=True)\n",
    "\n",
    "city_search_df = pd.DataFrame(columns=[\"airport_code\",\"state_id\",\"county_id\",\"city_id\",\"fips\",\"population\"])\n",
    "for ind,row in airport_prop_df.iterrows():\n",
    "    city = row[\"city_name\"]\n",
    "    target_lat = row[\"latitude\"]\n",
    "    target_lng = row[\"longitude\"]\n",
    "    try:      \n",
    "        target_cities = us_cities_df[us_cities_df[\"city_name\"]==city]\n",
    "        \n",
    "        if not (target_cities.shape[0] == 1): # If there are multiple city with same name\n",
    "            def calc_dis(input_):\n",
    "                err = abs(target_lat - input_[\"lat\"]) + abs(target_lng - input_[\"lng\"])\n",
    "                return err\n",
    "            target_cities.loc[:,\"error\"] = (target_cities.apply(calc_dis,axis=1))\n",
    "            target_city = target_cities.sort_values(by=\"error\").iloc[0]\n",
    "            \n",
    "            assert target_city[\"error\"] < 1.5 # assert the error should be <1.5 degs ~ 40 miles.\n",
    "            \n",
    "            target_city = target_city.drop([\"error\"])\n",
    "        elif (target_cities.shape[0] >= 1):\n",
    "            target_city = target_cities.iloc[0]\n",
    "        \n",
    "        county = str(target_city[\"county_fips\"])\n",
    "        if (len(county)==1):\n",
    "            county = \"00\" + county\n",
    "        elif (len(county)==2):\n",
    "            county = \"0\" + county\n",
    "\n",
    "        city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":target_city[\"state_id\"],\"county_id\":county,\"city_id\":target_city.name,\"fips\":target_city[\"fips\"],\"population\":target_city[\"population\"]},ignore_index=True)        \n",
    "    except:\n",
    "        try:\n",
    "            def calc_dis(input_):\n",
    "                err = abs(target_lat - input_[\"lat\"]) + abs(target_lng - input_[\"lng\"])\n",
    "                return err\n",
    "            us_cities_df_copy = us_cities_df\n",
    "            us_cities_df_copy.loc[:,\"error\"] = (us_cities_df.apply(calc_dis,axis=1))\n",
    "            target_city = us_cities_df_copy.sort_values(by=\"error\").iloc[0]\n",
    "            assert target_city[\"error\"] < 1.5\n",
    "            county = str(target_city[\"county_fips\"])\n",
    "            if (len(county)==1):\n",
    "                county = \"00\" + county\n",
    "            elif (len(county)==2):\n",
    "                county = \"0\" + county\n",
    "            city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":target_city[\"state_id\"],\"county_id\":county,\"city_id\":target_city.name,\"fips\":target_city[\"fips\"],\"population\":target_city[\"population\"]},ignore_index=True)\n",
    "        except:\n",
    "            print(\"No data for \",city)\n",
    "            city_search_df = city_search_df.append({\"airport_code\":ind,\"state_id\":np.nan,\"county_id\":np.nan,\"city_id\":np.nan,\"fips\":np.nan,\"population\":np.nan},ignore_index=True)\n",
    "\n",
    "#special case for DC\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    if (row[\"airport_code\"]==\"DCA\"):\n",
    "        city_search_df.iloc[ind][\"state_id\"]=\"MD\"\n",
    "        city_search_df.iloc[ind][\"county_id\"]=\"511\"\n",
    "\n",
    "# Remove Hawaii and Alaska\n",
    "city_search_df = city_search_df[(city_search_df[\"state_id\"]!=\"HI\") & (city_search_df[\"state_id\"]!=\"AK\")]\n",
    "airport_prop_df = airport_prop_df.merge(city_search_df.set_index(\"airport_code\").loc[:,[\"city_id\",\"fips\",\"population\",\"state_id\"]],left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Download and process temperature and precipitation data\n",
    "From https://www.ncdc.noaa.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_climate_data(state,county,year):\n",
    "    save_path = \"datasets/original/weather/\"\n",
    "    fname = state + county + \"_\" + str(year) + \".csv\"\n",
    "    if (len(glob.glob(save_path + fname))==0):\n",
    "        URL = \"https://www.ncdc.noaa.gov/cag/county/time-series/{}-{}-{}-all-1-2000-2020.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state,county,\"tavg\")\n",
    "        r = requests.get(URL)\n",
    "        file = wget.download(URL,out=save_path + \"tavg/tavg_\" + fname)\n",
    "        URL = \"https://www.ncdc.noaa.gov/cag/county/time-series/{}-{}-{}-all-1-2000-2020.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state,county,\"pcp\")\n",
    "        r = requests.get(URL)\n",
    "        file = wget.download(URL,out=save_path + \"pcp/pcp_\" + fname)\n",
    "\n",
    "        tavg_df = pd.read_csv(save_path + \"tavg/tavg_\" + fname).iloc[4:]\n",
    "        tavg_df.columns=[\"date\",\"tavg\",\"comp\"]\n",
    "        tavg = tavg_df.set_index(\"date\")[\"tavg\"]\n",
    "\n",
    "        pcp_df = pd.read_csv(save_path + \"pcp/pcp_\" + fname).iloc[4:]\n",
    "        pcp_df.columns=[\"date\",\"pcp\",\"comp\"]\n",
    "        pcp = pcp_df.set_index(\"date\")[\"pcp\"]\n",
    "\n",
    "        pd.concat([tavg, pcp], axis=1).to_csv(save_path + fname)\n",
    "        time.sleep(1) # not requesting too frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_search_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "# Download data according to city_search_df\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    try:\n",
    "        download_climate_data(row[\"state_id\"],row[\"county_id\"],2018)\n",
    "        download_climate_data(row[\"state_id\"],row[\"county_id\"],2019)\n",
    "    except:\n",
    "        print(row)\n",
    "    counter+=1\n",
    "    print(\"progress: {:.2f}%   Just done: {}\".format(100 * counter / city_search_df.shape[0],row[\"airport_code\"]),end=\"\\r\")\n",
    "\n",
    "# Process downloaded data\n",
    "temp_pcp_df = pd.DataFrame(columns=[\"airport_code\",\"temp_avg\",\"pcp_avg\"])\n",
    "for ind,row in city_search_df.iterrows():\n",
    "    state = row[\"state_id\"]\n",
    "    county = row[\"county_id\"]\n",
    "    save_path = \"datasets/original/weather/\"\n",
    "    years = [2018,2019]\n",
    "    \n",
    "    try:\n",
    "        tavg = 0\n",
    "        pcp = 0\n",
    "        for year in years:\n",
    "            fname = state + county + \"_\" + str(year) + \".csv\"\n",
    "            temp_pcp = pd.read_csv(save_path + fname)\n",
    "            tavg += temp_pcp.mean()[\"tavg\"]\n",
    "            pcp += temp_pcp.mean()[\"pcp\"]\n",
    "        tavg /= len(years)\n",
    "        pcp /= len(years)\n",
    "        \n",
    "        temp_pcp_df = temp_pcp_df.append({\"airport_code\":row[\"airport_code\"],\"temp_avg\":tavg,\"pcp_avg\":pcp},ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        temp_pcp_df = temp_pcp_df.append({\"airport_code\":row[\"airport_code\"],\"temp_avg\":np.nan,\"pcp_avg\":np.nan},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_prop_df = airport_prop_df.merge(temp_pcp_df.set_index(\"airport_code\"),left_index=True,right_index=True)\n",
    "airport_prop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Bird Strike dataset - strike_average, damage_average\n",
    "Clean `datasets/original/airports/Bird Strikes.xlsx`. Merge `bird_strike_avg_df` and `unpleasant_airport_code_df` to get `bird_strike_final_df`. Export it as `bird_strike.csv`.There's no airport code in `Bird Strikes.xlsx`, so additional dataset that contains airport name and airport code is used to link them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_strike_df = pd.read_excel(\"datasets/original/airport/Bird Strikes.xlsx\") #data6\n",
    "airport_name_df = pd.read_excel(\"datasets/original/airport/airportcode.xlsx\") #data7 \n",
    "bird_strike_df = bird_strike_df[[\"Airport: Name\", \"Effect: Indicated Damage\"]]\n",
    "bird_strike_df = bird_strike_df.rename(columns = {\"Airport: Name\": \"airport_name\", \"Effect: Indicated Damage\":\"bird_strike_effect\"})\n",
    "bird_strike_df = bird_strike_df.dropna()\n",
    "bird_strike_df = bird_strike_df.reset_index(drop = True)\n",
    "bird_strike_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_name_df = airport_name_df.dropna()\n",
    "airport_name_df = airport_name_df.reset_index(drop = True)\n",
    "airport_name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_airport_name(string):\n",
    "    \n",
    "    string = string.lower()\n",
    "    string = string.strip()\n",
    "    if 'intl' in string:\n",
    "        string = string.replace('intl', '')\n",
    "    if 'arpt' in string:\n",
    "        string = string.replace('arpt', '')\n",
    "    if 'regional' in string:\n",
    "        string = string.replace('regional', '')\n",
    "    if 'airport' in string:\n",
    "        string = string.replace('airport', '')\n",
    "    if 'sunport' in string:\n",
    "        string = string.replace('sunport', '')\n",
    "    if 'international' in string:\n",
    "        string = string.replace('international', '')\n",
    "    if 'intercontinental' in string:\n",
    "        string = string.replace('intercontinental', '')\n",
    "    else:\n",
    "        output = string\n",
    "        \n",
    "    string = string = string.strip()\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_strike_df['airport_name'] = bird_strike_df['airport_name'].apply(standardize_airport_name)\n",
    "airport_name_df['airport_name'] = airport_name_df['airport_name'].apply(standardize_airport_name)\n",
    "\n",
    "def check_strike (string):\n",
    "    return 1\n",
    "\n",
    "def check_damage (string):\n",
    "    if 'Caused' in string:\n",
    "        output = 1\n",
    "    else:\n",
    "        output = 0\n",
    "    return output\n",
    "\n",
    "bird_strike_df['strike'] = bird_strike_df['bird_strike_effect'].apply(check_strike)\n",
    "bird_strike_df['damage'] = bird_strike_df['bird_strike_effect'].apply(check_damage)\n",
    "bird_strike_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_strike = bird_strike_df.groupby('airport_name').agg({'strike':['sum']})\n",
    "grouped_strike = grouped_strike.reset_index()\n",
    "grouped_damage = bird_strike_df.groupby('airport_name').agg({'damage':['sum']})\n",
    "\n",
    "bird_strike_sum_df = pd.merge(grouped_strike, grouped_damage, on='airport_name')\n",
    "bird_strike_sum_df.columns = ['airport_name', 'strike_sum','damage_sum']\n",
    "\n",
    "bird_strike_avg_df = pd.merge(airport_name_df, bird_strike_sum_df, on='airport_name')\n",
    "\n",
    "def average_sum(input):\n",
    "    output = input/(2011 - 2000 + 1)\n",
    "    return output\n",
    "\n",
    "bird_strike_avg_df['strike_avg'] = bird_strike_avg_df['strike_sum'].apply(average_sum)\n",
    "bird_strike_avg_df['damage_avg'] = bird_strike_avg_df['damage_sum'].apply(average_sum)\n",
    "bird_strike_avg_df = bird_strike_avg_df.drop(columns = ['strike_sum', 'damage_sum'])\n",
    "\n",
    "bird_strike_avg_df= bird_strike_avg_df.drop(columns = [\"airport_name\"])\n",
    "bird_strike_avg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We later need to merge this dataframe to `airport_prop_df`. However, not all airports have bird strike. Therefore, we used a right merge(that kept all rows from `airport_prop_df`) and filled nan with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_strike_final_df = pd.merge(bird_strike_avg_df, unpleasant_airport_code_df.reset_index(), how='right')\n",
    "# unpleasant_airport_code_df have the same index as airport_prop_df\n",
    "bird_strike_final_df = bird_strike_final_df.fillna(0)\n",
    "bird_strike_final_df = bird_strike_final_df.set_index(\"airport_code\")\n",
    "bird_strike_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Runways dataset - runway length, width, count\n",
    "Clean `datasets/original/airport/runways.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_runways_df=pd.read_csv(\"datasets/original/airport/runways.csv\")\n",
    "airport_runways_df = airport_runways_df.rename(columns = {\"airport_ident\":\"code4\"})\n",
    "airport_runways_df = pd.merge(airport_runways_df, airport_loc_df.reset_index(), how = 'inner', on = 'code4')\n",
    "airport_runways_df = airport_runways_df[[\"code4\", \"airport_code\", \"length_ft\", \"width_ft\"]]\n",
    "airport_runways_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runways_sum_df = airport_runways_df.groupby(['airport_code']).sum()\n",
    "runways_mean_df = airport_runways_df.groupby(['airport_code']).mean()\n",
    "runways_count_df = airport_runways_df.groupby(['airport_code']).count()\n",
    "runway_final_df = runways_sum_df[\"length_ft\"].to_frame().join(runways_mean_df[\"width_ft\"].to_frame())\n",
    "runway_final_df[\"count\"] = runways_count_df[\"length_ft\"]\n",
    "runway_final_df.columns = [\"length_ft_sum\", \"width_ft_avg\",\"runway_count\"]\n",
    "\n",
    "runway_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 enplanements dataset - enplanements\n",
    "Clean `commercial_service_enplanements.xlsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enplanements_df = pd.read_excel('datasets/original/city/commercial_service_enplanements.xlsx')\n",
    "enplanements_df = enplanements_df.rename(columns = {\"Locid\":\"airport_code\", \"% Change\":\"enplanement_change\", \"CY 18 Enplanements\":\"enplanements_18\", \"CY 17 Enplanements\":\"enplanements_17\"})\n",
    "enplanements_df = enplanements_df.set_index(\"airport_code\")\n",
    "enplanements_df = enplanements_df[[\"enplanements_17\", \"enplanements_18\"]]\n",
    "\n",
    "temp_enplanements = enplanements_df[\"enplanements_17\"]+enplanements_df[\"enplanements_18\"]*0.5\n",
    "enplanements_df[\"enplanements\"] = temp_enplanements\n",
    "enplanements_df = enplanements_df[[\"enplanements\"]]\n",
    "enplanements_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Cleaning step: merging all dataframes together to get X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_X_df = pd.concat([X,airport_prop_df, bird_strike_final_df, enplanements_df, runway_final_df], axis=1, sort=False)\n",
    "merged_X_df = merged_X_df.dropna()\n",
    "merged_X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.merge(merged_X_df[[]],how=\"right\",left_index=True,right_index=True)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_X_df.isna().sum().sum() + Y.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Departure ~ Arrival Performance Corrolation\n",
    "\n",
    "As mentioned previously, the 'y' datasets focus on both departure performance and arrival performance of all recorded flights in 2018. Are our datasets consistent enough between two performances so that the delay/cancellation/diversion rates we calculated for each airport are not biased in any way? We tested this by looking at the correlation between total departure count and total arrival count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_arr_corr_df = merged_X_df.loc[:,[\"total_departure\",\"total_arrival\",\"total_departure_lg10\",\"total_arrival_lg10\",\n",
    "                                     \"averge_departure_distance\",\"averge_arrival_distance\"]].merge(Y.loc[:,[\"average_departure_delay\",\"average_arrival_delay\"]],left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pd.plotting.scatter_matrix(dep_arr_corr_df.loc[:,[\"total_departure_lg10\",\"total_arrival_lg10\"]],figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_arr_corr_df.loc[:,[\"total_departure_lg10\",\"total_arrival_lg10\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the corrolation values, a nearly perfect linear corrolation was found between total departure count and total arrival count. This proves that every plane that went into an airport also came out eventually. **Therefore, this (almost) perfect linear relationship indicates that our 'y' datasets capture flights completely with no outliers**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "st_dep,p_dep = stats.normaltest(dep_arr_corr_df[\"total_departure_lg10\"].values)\n",
    "st_arr,p_arr = stats.normaltest(dep_arr_corr_df[\"total_arrival_lg10\"].values)\n",
    "print(\"P-value for departure_lg10 is :\\t\",p_dep,\"is it normal under alpha =\",alpha,\"? - \",p_dep>alpha)\n",
    "print(\"P-value for arrival_lg10 is :\\t\",p_arr,\"is it normal under alpha =\",alpha,\"? - \",p_arr>alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are:\n",
    "- **linear related**: The correlation extremely close to 1.\n",
    "- **normally distributed**: as tested\n",
    "- **homoscedasic**: The graph looks consistent.\n",
    "- **not auto-corrolated**: Though arrival depends on departure, the departure count of one airport is unlikely to affect the departure count of another airport.\n",
    "- **multicollinearity**: This is not a multivariate regression.\n",
    "\n",
    "Therefore we can conduct a linear regression to see whether there is a biased departure and arrival recording. This is different from above: no outliers indicated no individual airport have significantly different departure / arrival ratio from other airports. Now what we want to test is the ratio 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_1,predictors_1 = patsy.dmatrices('total_arrival_lg10 ~ total_departure_lg10',dep_arr_corr_df)\n",
    "mod_1 = sm.OLS(outcome_1, predictors_1)\n",
    "res_1 = mod_1.fit()\n",
    "print(res_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect size is **0.9999**, and we have 95% confidence range covers exactly **1.000**. i.e. For any increase in log10(departure), we have >97.5% confidence that there will be 1.000 \\* that amount of increase in log10(arrival). Although the intercept is 0.0004, the 95% confidence range covers 0, so we can conclude that the error is small enough to ignore. **Therefore, this validates 1) our datasets are complete; 2) departure and arrival performances are not biased.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.scatter(dep_arr_corr_df[\"averge_departure_distance\"],dep_arr_corr_df[\"averge_arrival_distance\"])\n",
    "plt.title(\"corrolation between average departure / arrival distance\")\n",
    "plt.xlabel(\"average departure distance\")\n",
    "plt.ylabel(\"average arrival distance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we found an overall linear relationship between average departure distance and average arrival distance. This could be possibly explained by the fact that for any pair of cities, there often exist airlines that go to and come back from destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.scatter(dep_arr_corr_df[\"average_departure_delay\"],dep_arr_corr_df[\"average_arrival_delay\"])\n",
    "plt.title(\"corrolation between average departure / arrival delay\")\n",
    "plt.xlabel(\"average departure delay\")\n",
    "plt.ylabel(\"average arrival delay\");\n",
    "print(\"Corrolation: \",np.corrcoef(dep_arr_corr_df[\"average_departure_delay\"],dep_arr_corr_df[\"average_arrival_delay\"])[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph, departure delay and arrival delay are weakly correlated, which further proves the necessity of **analyzing departure and arrival performance separately**.\n",
    "\n",
    "\n",
    "We can also see from the graph(the axes) that the scales of departure and arrival delay are different - arrival delay is slightly smaller than departure delay. We need to do a linear regression to find out the coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = dep_arr_corr_df[\"average_departure_delay\"]\n",
    "dat2 = dep_arr_corr_df[\"average_arrival_delay\"]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,9))\n",
    "ax1.hist(dat1,bins=25)\n",
    "plt.title(\"aveage departure delay & arrival delay distribution\")\n",
    "ax1.set_xlabel(\"departure delay\");\n",
    "ax1.set_label(\"count\");\n",
    "ax2.hist(dat2,bins=25)\n",
    "ax2.set_xlabel(\"departure delay\");\n",
    "ax2.set_label(\"count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "st_dep,p_dep = stats.normaltest(dep_arr_corr_df[\"average_departure_delay\"].values)\n",
    "st_arr,p_arr = stats.normaltest(dep_arr_corr_df[\"average_arrival_delay\"].values)\n",
    "print(\"P-value for average_departure_delay is :\\t\",p_dep,\"is it normal under alpha =\",alpha,\"? - \",p_dep>alpha)\n",
    "print(\"P-value for average_arrival_delay is :\\t\",p_arr,\"is it normal under alpha =\",alpha,\"? - \",p_arr>alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the distribution looks fairly normal, the test result rejects such assupmtion. After mutliple attempts, we could not find a way to transform it. Linear regression does not work well here. We then ask the question: Is there a significant difference between the mean of departure and arrival delay? If so, how large is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = (dep_arr_corr_df[\"average_departure_delay\"] - dep_arr_corr_df[\"average_arrival_delay\"]).values;\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,9))\n",
    "plt.title(\"aveage departure delay minus arrival delay (delta delay) distribution\")\n",
    "ax1.hist(dat,bins=25)\n",
    "ax1.set_xlabel(\"delta delay / min\");\n",
    "ax1.set_label(\"count\");\n",
    "ax1.axvline(dat.mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "ax1.text(dat.mean()*1.1, 70*0.9, 'Mean: {:.2f}'.format(dat.mean()))\n",
    "ax1.text(dat.mean()*1.1, 70*0.8, 'Std: {:.2f}'.format(dat.std()))\n",
    "\n",
    "ax2.boxplot(dat)\n",
    "ax2.set_xlabel(\"delta delay / min\");\n",
    "ax2.set_label(\"count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, the mean of the difference (departure delay - arrival delay) is **4.48** minutes with a standard deviation of **4.35**. We concluded that **on average, arrival delay is 4.48 minutes less than departure delay**. We figured this difference have a reasonable explanation: After delayed by the originating airport, pilots would typically cruise at a faster speed to catch up with the schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial distribution\n",
    "Are our predictor variables ('x') correlated with location? Are our outcome variables ('y') correlated with location? In other words, is location a possible covariate behind the correlation between our predictor and outcome variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bubble_map(df,column,limits,color,title,scale=1):\n",
    "\n",
    "    if not (column in df.columns):\n",
    "        print(\"column \\\"\"+column+\"\\\"\",\"not found in dataframe\")\n",
    "        return\n",
    "    \n",
    "    if not ('longitude' in df.columns and 'latitude' in df.columns):\n",
    "        print(\"longitude or latitude not present in df\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        n = int(limits) + 1\n",
    "        lims = np.linspace(df[column].min()-abs(df[column].min())*0.001,df[column].max() + abs(df[column].max())*0.001,n)\n",
    "        # subtract / add a small number to aviod rouding error that will left out max / min\n",
    "        limits = list()\n",
    "        for i in range(n):\n",
    "            limits.append((lims[i],lims[i+1]))\n",
    "        \n",
    "    except:\n",
    "        _ = 0 # do nothing\n",
    "    \n",
    "    \n",
    "    if not (limits[0][0] <= df[column].min()):\n",
    "        print(\"lower limit is higher then some of the rows\")\n",
    "        print(\"df[\" + column + \"].min()=\",df[column].min())\n",
    "    \n",
    "    if not (limits[len(limits)-1][1] >= df[column].max()):\n",
    "        print(\"upper limit is lower then some of the rows\")\n",
    "        print(\"df[\" + column + \"].max()=\",df[column].max())\n",
    "    \n",
    "    #preprocess color\n",
    "    colorseries={\n",
    "        \"blue\":[\"#f7fbff\", \"#ebf3fb\", \"#d2e3f3\", \"#c6dbef\", \"#b3d2e9\", \"#9ecae1\",\n",
    "                \"#85bcdb\", \"#6baed6\", \"#57a0ce\", \"#3082be\", \"#2171b5\", \"#1361a9\",\n",
    "                \"#08519c\", \"#0b4083\", \"#08306b\"],\n",
    "        \"gray\":[\"#ffffff\",\"#eeeeee\",\"#dddddd\",\"#cccccc\",\"#bbbbbb\",\"#aaaaaa\",\"#999999\",\"#888888\"],\n",
    "        \"thermal\":[\"#00000a\",\"#000014\",\"#00001e\",\"#000025\",\"#00002a\",\"#00002e\",\"#000032\",\"#000036\",\"#00003a\",\"#00003e\",\"#000042\",\"#000046\",\"#00004a\",\"#00004f\",\"#000052\",\"#010055\",\"#010057\",\"#020059\",\"#02005c\",\"#03005e\",\"#040061\",\"#040063\",\"#050065\",\"#060067\",\"#070069\",\"#08006b\",\"#09006e\",\"#0a0070\",\"#0b0073\",\"#0c0074\",\"#0d0075\",\"#0d0076\",\"#0e0077\",\"#100078\",\"#120079\",\"#13007b\",\"#15007c\",\"#17007d\",\"#19007e\",\"#1b0080\",\"#1c0081\",\"#1e0083\",\"#200084\",\"#220085\",\"#240086\",\"#260087\",\"#280089\",\"#2a0089\",\"#2c008a\",\"#2e008b\",\"#30008c\",\"#32008d\",\"#34008e\",\"#36008e\",\"#38008f\",\"#390090\",\"#3b0091\",\"#3c0092\",\"#3e0093\",\"#3f0093\",\"#410094\",\"#420095\",\"#440095\",\"#450096\",\"#470096\",\"#490096\",\"#4a0096\",\"#4c0097\",\"#4e0097\",\"#4f0097\",\"#510097\",\"#520098\",\"#540098\",\"#560098\",\"#580099\",\"#5a0099\",\"#5c0099\",\"#5d009a\",\"#5f009a\",\"#61009b\",\"#63009b\",\"#64009b\",\"#66009b\",\"#68009b\",\"#6a009b\",\"#6c009c\",\"#6d009c\",\"#6f009c\",\"#70009c\",\"#71009d\",\"#73009d\",\"#75009d\",\"#77009d\",\"#78009d\",\"#7a009d\",\"#7c009d\",\"#7e009d\",\"#7f009d\",\"#81009d\",\"#83009d\",\"#84009d\",\"#86009d\",\"#87009d\",\"#89009d\",\"#8a009d\",\"#8b009d\",\"#8d009d\",\"#8f009c\",\"#91009c\",\"#93009c\",\"#95009c\",\"#96009b\",\"#98009b\",\"#99009b\",\"#9b009b\",\"#9c009b\",\"#9d009b\",\"#9f009b\",\"#a0009b\",\"#a2009b\",\"#a3009b\",\"#a4009b\",\"#a6009a\",\"#a7009a\",\"#a8009a\",\"#a90099\",\"#aa0099\",\"#ab0099\",\"#ad0099\",\"#ae0198\",\"#af0198\",\"#b00198\",\"#b00198\",\"#b10197\",\"#b20197\",\"#b30196\",\"#b40296\",\"#b50295\",\"#b60295\",\"#b70395\",\"#b80395\",\"#b90495\",\"#ba0495\",\"#ba0494\",\"#bb0593\",\"#bc0593\",\"#bd0593\",\"#be0692\",\"#bf0692\",\"#bf0692\",\"#c00791\",\"#c00791\",\"#c10890\",\"#c10990\",\"#c20a8f\",\"#c30a8e\",\"#c30b8e\",\"#c40c8d\",\"#c50c8c\",\"#c60d8b\",\"#c60e8a\",\"#c70f89\",\"#c81088\",\"#c91187\",\"#ca1286\",\"#ca1385\",\"#cb1385\",\"#cb1484\",\"#cc1582\",\"#cd1681\",\"#ce1780\",\"#ce187e\",\"#cf187c\",\"#cf197b\",\"#d01a79\",\"#d11b78\",\"#d11c76\",\"#d21c75\",\"#d21d74\",\"#d31e72\",\"#d32071\",\"#d4216f\",\"#d4226e\",\"#d5236b\",\"#d52469\",\"#d62567\",\"#d72665\",\"#d82764\",\"#d82862\",\"#d92a60\",\"#da2b5e\",\"#da2c5c\",\"#db2e5a\",\"#db2f57\",\"#dc2f54\",\"#dd3051\",\"#dd314e\",\"#de324a\",\"#de3347\",\"#df3444\",\"#df3541\",\"#df363d\",\"#e0373a\",\"#e03837\",\"#e03933\",\"#e13a30\",\"#e23b2d\",\"#e23c2a\",\"#e33d26\",\"#e33e23\",\"#e43f20\",\"#e4411d\",\"#e4421c\",\"#e5431b\",\"#e54419\",\"#e54518\",\"#e64616\",\"#e74715\",\"#e74814\",\"#e74913\",\"#e84a12\",\"#e84c10\",\"#e84c0f\",\"#e94d0e\",\"#e94d0d\",\"#ea4e0c\",\"#ea4f0c\",\"#eb500b\",\"#eb510a\",\"#eb520a\",\"#eb5309\",\"#ec5409\",\"#ec5608\",\"#ec5708\",\"#ec5808\",\"#ed5907\",\"#ed5a07\",\"#ed5b06\",\"#ee5c06\",\"#ee5c05\",\"#ee5d05\",\"#ee5e05\",\"#ef5f04\",\"#ef6004\",\"#ef6104\",\"#ef6204\",\"#f06303\",\"#f06403\",\"#f06503\",\"#f16603\",\"#f16603\",\"#f16703\",\"#f16803\",\"#f16902\",\"#f16a02\",\"#f16b02\",\"#f16b02\",\"#f26c01\",\"#f26d01\",\"#f26e01\",\"#f36f01\",\"#f37001\",\"#f37101\",\"#f37201\",\"#f47300\",\"#f47400\",\"#f47500\",\"#f47600\",\"#f47700\",\"#f47800\",\"#f47a00\",\"#f57b00\",\"#f57c00\",\"#f57e00\",\"#f57f00\",\"#f68000\",\"#f68100\",\"#f68200\",\"#f78300\",\"#f78400\",\"#f78500\",\"#f78600\",\"#f88700\",\"#f88800\",\"#f88800\",\"#f88900\",\"#f88a00\",\"#f88b00\",\"#f88c00\",\"#f98d00\",\"#f98d00\",\"#f98e00\",\"#f98f00\",\"#f99000\",\"#f99100\",\"#f99200\",\"#f99300\",\"#fa9400\",\"#fa9500\",\"#fa9600\",\"#fb9800\",\"#fb9900\",\"#fb9a00\",\"#fb9c00\",\"#fc9d00\",\"#fc9f00\",\"#fca000\",\"#fca100\",\"#fda200\",\"#fda300\",\"#fda400\",\"#fda600\",\"#fda700\",\"#fda800\",\"#fdaa00\",\"#fdab00\",\"#fdac00\",\"#fdad00\",\"#fdae00\",\"#feaf00\",\"#feb000\",\"#feb100\",\"#feb200\",\"#feb300\",\"#feb400\",\"#feb500\",\"#feb600\",\"#feb800\",\"#feb900\",\"#feb900\",\"#feba00\",\"#febb00\",\"#febc00\",\"#febd00\",\"#febe00\",\"#fec000\",\"#fec100\",\"#fec200\",\"#fec300\",\"#fec400\",\"#fec500\",\"#fec600\",\"#fec700\",\"#fec800\",\"#fec901\",\"#feca01\",\"#feca01\",\"#fecb01\",\"#fecc02\",\"#fecd02\",\"#fece03\",\"#fecf04\",\"#fecf04\",\"#fed005\",\"#fed106\",\"#fed308\",\"#fed409\",\"#fed50a\",\"#fed60a\",\"#fed70b\",\"#fed80c\",\"#fed90d\",\"#ffda0e\",\"#ffda0e\",\"#ffdb10\",\"#ffdc12\",\"#ffdc14\",\"#ffdd16\",\"#ffde19\",\"#ffde1b\",\"#ffdf1e\",\"#ffe020\",\"#ffe122\",\"#ffe224\",\"#ffe226\",\"#ffe328\",\"#ffe42b\",\"#ffe42e\",\"#ffe531\",\"#ffe635\",\"#ffe638\",\"#ffe73c\",\"#ffe83f\",\"#ffe943\",\"#ffea46\",\"#ffeb49\",\"#ffeb4d\",\"#ffec50\",\"#ffed54\",\"#ffee57\",\"#ffee5b\",\"#ffee5f\",\"#ffef63\",\"#ffef67\",\"#fff06a\",\"#fff06e\",\"#fff172\",\"#fff177\",\"#fff17b\",\"#fff280\",\"#fff285\",\"#fff28a\",\"#fff38e\",\"#fff492\",\"#fff496\",\"#fff49a\",\"#fff59e\",\"#fff5a2\",\"#fff5a6\",\"#fff6aa\",\"#fff6af\",\"#fff7b3\",\"#fff7b6\",\"#fff8ba\",\"#fff8bd\",\"#fff8c1\",\"#fff8c4\",\"#fff9c7\",\"#fff9ca\",\"#fff9cd\",\"#fffad1\",\"#fffad4\",\"#fffbd8\",\"#fffcdb\",\"#fffcdf\",\"#fffde2\",\"#fffde5\",\"#fffde8\",\"#fffeeb\",\"#fffeee\",\"#fffef1\",\"#fffef4\",\"#fffff6\"],\n",
    "        \"black\":[\"#bbbbbb\",\"#aaaaaa\",\"#999999\",\"#888888\",\"#777777\",\"#666666\",\"#555555\",\"#444444\",\"#333333\",\"#222222\",\"#111111\",\"#000000\"]\n",
    "    }\n",
    "    colors = list()\n",
    "    inds = list(np.linspace(0,len(colorseries[color])-1,len(limits)).astype(\"int\"))\n",
    "    for i in inds:\n",
    "        colors.append(colorseries[color][i])\n",
    "        \n",
    "    scale *= 250 / (df[column].max() - df[column].min())\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for i in range(len(limits)):\n",
    "        lim = limits[i]\n",
    "        df_sub = df[(lim[0]<=df[column]) & (df[column]<lim[1])]\n",
    "        fig.add_trace(go.Scattergeo(\n",
    "            locationmode = 'USA-states',\n",
    "            lon = df_sub['longitude'],\n",
    "            lat = df_sub['latitude'],\n",
    "            text = df_sub.index,\n",
    "            marker = dict(\n",
    "                size = (df_sub[column] - df[column].min())*scale, # ensure size is positive\n",
    "                color = colors[i],\n",
    "                line_color='rgb(40,40,40)',\n",
    "                line_width=0,\n",
    "                sizemode = 'area'\n",
    "            ),\n",
    "            name = '{0:.3g} - {1:.3g}'.format(lim[0],lim[1])))\n",
    "\n",
    "    fig.update_layout(\n",
    "            title_text = title,\n",
    "            showlegend = True,\n",
    "            geo = dict(\n",
    "                scope = 'usa',\n",
    "                landcolor = 'rgb(217, 217, 217)',\n",
    "            )\n",
    "        )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = generate_bubble_map(Y.merge(merged_X_df[[\"latitude\",\"longitude\"]],left_index=True,right_index=True),\n",
    "                          column=\"average_departure_delay\",\n",
    "                          limits=[(-15,0),(0,5),(5,10),(10,20),(20,40)],\n",
    "                          color=\"blue\",\n",
    "                          title='2018 United States airport departure delay')\n",
    "#fig1.show()\n",
    "fig1.write_image(\"images/dep_delay.png\",width=960,height=600)\n",
    "fig2 = generate_bubble_map(Y.merge(merged_X_df[[\"latitude\",\"longitude\"]],left_index=True,right_index=True),\n",
    "                          column=\"average_arrival_delay\",\n",
    "                          limits=[(-15,0),(0,5),(5,10),(10,20),(20,40)],\n",
    "                          color=\"blue\",\n",
    "                          title='2018 United States airport arrival delay')\n",
    "#fig2.show()\n",
    "fig2.write_image(\"images/arr_delay.png\",width=960,height=600)\n",
    "display(Image(filename='images/dep_delay.png'))\n",
    "display(Image(filename='images/arr_delay.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, we found:\n",
    "- Arrival delay is overall less than departure delay (consistent with previous finding).\n",
    "     \n",
    "- The triangular region (from center of Nevada to U.S. - Canadian boarder) in the upper left of the map has overall less departure and arrival delay. \n",
    "    \n",
    "- Both departure and arrival delays tend to distribute along the coastline.\n",
    "\n",
    "These findings lead us to raise question: Is there a significant difference of departure/arrival delay between airports that are located at a coastal state and ones that are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_coastline\n",
    "States_coast = [\"FL\",\"CA\",\"LA\",\"TX\",\"NC\",\"OR\",\"ME\",\"MA\",\"SC\",\"WA\",\"NJ\",\"NY\",\"VA\",\"GA\",\"CT\",\"AL\",\"MS\",\"RI\",\"MD\",\"DE\",\"NH\"]\n",
    "combined = Y.merge(merged_X_df,left_index=True,right_index=True)\n",
    "def is_coast(input_state):\n",
    "    if input_state in States_coast:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "coast_dep = combined[combined[\"state_id\"].apply(is_coast)][\"average_departure_delay\"]\n",
    "non_coast_dep = combined[~combined[\"state_id\"].apply(is_coast)][\"average_departure_delay\"]\n",
    "\n",
    "coast_arr = combined[combined[\"state_id\"].apply(is_coast)][\"average_arrival_delay\"]\n",
    "non_coast_arr = combined[~combined[\"state_id\"].apply(is_coast)][\"average_arrival_delay\"]\n",
    "\n",
    "\n",
    "f, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2, 2, sharex=True,figsize=(16,9))\n",
    "ax1.hist(coast_dep,bins=25,)\n",
    "ax1.set_xlabel(\"coast departure delay / min\");\n",
    "ax1.set_label(\"count\");\n",
    "ax2.hist(coast_arr,bins=25)\n",
    "ax2.set_xlabel(\"coast arrival delay / min\");\n",
    "ax2.set_label(\"count\");\n",
    "\n",
    "ax3.hist(non_coast_dep,bins=25)\n",
    "ax3.set_xlabel(\"non-coast departure delay / min\");\n",
    "ax3.set_label(\"count\");\n",
    "ax4.hist(non_coast_arr,bins=25)\n",
    "ax4.set_xlabel(\"non-coast arrival delay / min\");\n",
    "ax4.set_label(\"count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do they have the same distribution? \n",
    "\n",
    "We chose to do a Kolmogorov-Smirnov(KS) test to see if the distribution of delay time for airports in coastal states and non-coastal states are different (under alpha = 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dep, pval_dep = stats.ks_2samp(coast_dep,non_coast_dep)\n",
    "stat_arr, pval_arr = stats.ks_2samp(coast_arr,non_coast_arr)\n",
    "\n",
    "print(\"P-value for coast / non-coast departure delay are different is :\\t\",pval_dep,\"is there a significent difference? - \",pval_dep<alpha)\n",
    "print(\"P-value for coast / non-coast arrival delay are different is :\\t\",pval_arr,\"is there a significent difference? - \",pval_arr<alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KS test yields the following results: **Under alpha = 0.01, we failed to reject that there is no significent difference between the distribution of coastal / non-coastal airports in terms of both departure and arrival delay.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As they have the same distribution, is there a significant difference between the mean delay of coastal and non-coastal airports?\n",
    "\n",
    "We chose to use the t-test, which assumes the mean of sample is normally distributed, but does not assume the sample itself is normally distributed. Therefore, according to the Central Limit theorem, means of samples of a finite variance approach a normal distribution regardless of the origional distribution. Since we have `N=312`, it's safe to assume the sample mean follows a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_val_dep,p_val_dep = stats.ttest_ind(coast_dep,non_coast_dep)\n",
    "t_val_arr,p_val_arr = stats.ttest_ind(coast_arr,non_coast_arr)\n",
    "print(\"For mean of average_departure/arrival_delay in costal and non-costal airports:\")\n",
    "print(\"P-value for difference in departure_delay mean is : {0:.3g} is there a significent difference under alpha =\".format(p_val_dep),alpha,\"? - \",p_val_dep<alpha)\n",
    "print(\"P-value for difference in arrival_delay mean is : {0:.3g} is there a significent difference under alpha =\".format(p_val_arr),alpha,\"? - \",p_val_arr<alpha)\n",
    "print(\"t-statistic for difference in departure_delay mean is : {0:.3g}, is it positive? - \".format(t_val_dep),t_val_dep>0)\n",
    "print(\"t-statistic for difference in arrival_delay mean is : {0:.3g}, is it positive? - \".format(t_val_arr),t_val_arr>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With t-test, we concluded that **there is a significant positive difference in mean of average arrival delay and average departure delay between coastal and non-coastal airports.** In short, **coastal airports do have significantly more departure and arrival delay.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = generate_bubble_map(Y.merge(merged_X_df[[\"latitude\",\"longitude\"]],left_index=True,right_index=True),\n",
    "                          column=\"average_departure_cancelled\",\n",
    "                          limits=8,\n",
    "                          color=\"blue\",\n",
    "                          title='2018 United States airport departure cancelled')\n",
    "#fig1.show()\n",
    "fig1.write_image(\"images/dep_cancel.png\",width=960,height=600)\n",
    "fig2 = generate_bubble_map(Y.merge(merged_X_df[[\"latitude\",\"longitude\"]],left_index=True,right_index=True),\n",
    "                          column=\"average_arrival_diverted\",\n",
    "                          limits=8,\n",
    "                          color=\"blue\",\n",
    "                          title='2018 United States airport arrival diverted')\n",
    "#fig2.show()\n",
    "fig2.write_image(\"images/arr_divert.png\",width=960,height=600)\n",
    "display(Image(filename='images/dep_cancel.png'))\n",
    "display(Image(filename='images/arr_divert.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The east coast have greater cancellation rates, which suggests that it may be correlated with longitude.\n",
    "\n",
    "- There is a outlier of diversion rate in the state Idaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outlier:\n",
    "merged_X_df[Y[\"average_arrival_diverted\"]==Y[\"average_arrival_diverted\"].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier have a reasonable enplanement number: 133281. As we inspected the raw data, there is no NAN for cancelled flights. Therefore we can't remove it from our dataset. However, there must be some sepcific reason behind this airport that does not adhere to overall trend. We re-plotted a map with SUN removed (but not from the original dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = generate_bubble_map(Y.merge(merged_X_df[[\"latitude\",\"longitude\"]],left_index=True,right_index=True).drop(\"SUN\",axis=0),\n",
    "                          column=\"average_arrival_diverted\",\n",
    "                          limits=8,\n",
    "                          color=\"blue\",\n",
    "                          title='2018 United States airport arrival diverted (removed SUN - for this map only)')\n",
    "#fig2.show()\n",
    "fig2.write_image(\"images/arr_divert_rm_outlier.png\",width=960,height=600)\n",
    "\n",
    "display(Image(filename='images/arr_divert_rm_outlier.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(merged_X_df,\n",
    "                          column=\"strike_avg\",\n",
    "                          limits=[(0,10),(10,20),(20,30),(30,40),(40,50),(50,60),(60,70)],\n",
    "                          color=\"blue\",\n",
    "                          title='United States bird strike average')\n",
    "fig.write_image(\"images/bird.png\",width=960,height=600)\n",
    "display(Image(filename='images/bird.png'))\n",
    "fig = generate_bubble_map(merged_X_df,\n",
    "                          column=\"damage_avg\",\n",
    "                          limits=10,\n",
    "                          color=\"blue\",\n",
    "                          title='United States bird damage average')\n",
    "fig.write_image(\"images/bird_dmg.png\",width=960,height=600)\n",
    "display(Image(filename='images/bird_dmg.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that bird strikes occur less frequenlty in high latitude areas and more frequently in low latitude areas. Thus it is reasonable to assume bird strikes may be correlated with latitude, or tempreature(climate).\n",
    "\n",
    "Now let's examine the distribution and correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(merged_X_df[[\"strike_avg\",\"damage_avg\",\"latitude\",\"altitude_ft\",\"temp_avg\",\"pcp_avg\"]],figsize=(16,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_X_df[[\"strike_avg\",\"damage_avg\",\"latitude\",\"altitude_ft\",\"temp_avg\",\"pcp_avg\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation shows **bird strike rate and bird damage rate are weakly correlated with latitude, altitude and tempreature**. There is a weak( ~0.1) negative correlation between latitude/altitude and bird strike/damage rate and a weak positive correlation between tempreature and bird strike/damage rate. There is no correlation between precipitation and bird strike/damage rate.\n",
    "\n",
    "Besides bird strike rate, there is a **strong (-0.94) negative correlation between tempreature and latitute** as we expected. A possible explanation could be birds perfer and gather in warmer habitats, increasing the possiblity of collision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_bubble_map(merged_X_df,\n",
    "                          column=\"pcp_avg\",\n",
    "                          limits=[(0,1.4),(1.4,2.8),(2.8,4.2),(4.2,5.6),(5.6,7.0)],\n",
    "                          color=\"blue\",\n",
    "                          title='United States precipitation average')\n",
    "#fig.show()\n",
    "fig.write_image(\"images/pcp.png\",width=960,height=600)\n",
    "display(Image(filename='images/pcp.png'))\n",
    "\n",
    "fig = generate_bubble_map(merged_X_df,\n",
    "                          column=\"temp_avg\",\n",
    "                          limits=10,\n",
    "                          color=\"thermal\",\n",
    "                          title='United States temperature average')\n",
    "fig.write_image(\"images/temp.png\",width=960,height=600)\n",
    "display(Image(filename='images/temp.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure validates our data by showing more percipitation in the east than west, and higher temperature in the south than the north, which is aligned with facts of the U.S. climate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check for our enplanements variable. \n",
    "We chose to take the log of enplanements since they are in different magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_X_df_temp = merged_X_df.copy()\n",
    "merged_X_df_temp[\"enplanements\"] = merged_X_df_temp[\"enplanements\"].apply(np.log10)\n",
    "fig = generate_bubble_map(merged_X_df_temp,\n",
    "                          column=\"enplanements\",\n",
    "                          limits=10,\n",
    "                          color=\"blue\",\n",
    "                          title='United States lg10(enplanements)')\n",
    "fig.write_image(\"images/enplanements.png\",width=960,height=600)\n",
    "display(Image(filename='images/enplanements.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region in upper left, where there is less delay, has fewer enplanements. This indicates that flight delay may be positively correlated with enplanements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
